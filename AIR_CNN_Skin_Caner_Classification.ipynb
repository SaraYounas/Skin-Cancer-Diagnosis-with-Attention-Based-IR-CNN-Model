{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNadK2dI6SVD"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O28ui2RZz_bn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation,GlobalAveragePooling2D,Concatenate ,AveragePooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "# from skimage.feature import greycomatrix, greycoprops\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import models, layers, regularizers\n",
        "import keras as keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, Dropout, concatenate\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import keras as keras\n",
        "from keras import backend\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, Concatenate, Lambda\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras import regularizers, activations\n",
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "from keras.models import Model\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation,GlobalAveragePooling2D,Concatenate,Lambda\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Input, GlobalAveragePooling2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTaoKT7QmBaJ",
        "outputId": "36cf9056-fc5f-474b-c6cb-c44d41ac3bf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: NVIDIA GeForce RTX 2080 Ti (UUID: GPU-f2f15110-8713-3371-414f-21e6c693f641)\n",
            "GPU 1: NVIDIA GeForce RTX 2080 Ti (UUID: GPU-ab7ed36d-8dc7-8e92-4f77-8333691f7112)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGE_76zRp-ib",
        "outputId": "254bda1c-36fc-46cd-dbdc-a38e9ceaad38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 Physical GPUs, 1 Logical GPU\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.set_visible_devices(gpus[1], 'GPU')\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QhdoZA9p-id",
        "outputId": "8e85660e-6809-4a73-b13f-2a15ef8d343e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Checking available GPUs\n",
        "import tensorflow as tf\n",
        "gpus = tf.config.list_physical_devices()\n",
        "gpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBf7d3I_p-ie",
        "outputId": "dc0fe749-4f4b-4919-c48d-902cfefb8bf6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "try:\n",
        "  # Disable all GPUS\n",
        "  tf.config.set_visible_devices([1], 'GPU')\n",
        "  visible_devices = tf.config.get_visible_devices()\n",
        "  for device in visible_devices:\n",
        "    assert device.device_type != 'GPU'\n",
        "except:\n",
        "  # Invalid device or cannot modify virtual devices once initialized.\n",
        "  pass\n",
        "\n",
        "visible_devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K-rD0BBgVMr",
        "outputId": "dcb81cd7-52b4-4c8a-c0e1-bb8d74df7aee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 Physical GPUs, 1 Logical GPU\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.set_visible_devices(gpus[1], 'GPU')\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iWp2VoZp-if",
        "outputId": "fd0abe02-68b5-4bd0-c77e-37a97a74b5ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
          ]
        }
      ],
      "source": [
        "config =  tf.config.set_visible_devices([], {'GPU': 1})\n",
        "print(visible_devices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i09br_OYQUpk"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0kEG_BffQww",
        "outputId": "2e948e5b-558f-468e-de2f-af5f18a1db3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded Successfully\n"
          ]
        }
      ],
      "source": [
        "## load data\n",
        "from numpy import load\n",
        "\n",
        "tr_d = load('D:/SaraYounas/Dataset/Data-1/Data-1.npy')\n",
        "tr_l = load('D:/SaraYounas/Dataset/Data-1/DataLabels-1.npy')\n",
        "print(\"Loaded Successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2egSGEmmS5j",
        "outputId": "6011d78a-b637-41e0-841e-c9ba7b505557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(17522, 224, 224, 3)\n",
            "(17522,)\n"
          ]
        }
      ],
      "source": [
        "#Confirming Shape\n",
        "Images = tr_d\n",
        "Labels = tr_l\n",
        "print(Images.shape)\n",
        "print(Labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bqCwDZkLcCe",
        "outputId": "a5a07132-44de-4db0-dacc-007418f7e0e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded Successfully\n"
          ]
        }
      ],
      "source": [
        "## load data\n",
        "from numpy import load\n",
        "\n",
        "test_d = load('D:/SaraYounas/Dataset/Data-2/Data-2.npy')\n",
        "test_l = load('D:/SaraYounas/Dataset/Data-2/DataLabels-2.npy')\n",
        "print(\"Loaded Successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_zdJXzJWqU0",
        "outputId": "c54c2dfb-a6d3-4d64-d9c9-f982bf1788f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7650, 224, 224, 3)\n",
            "(7650,)\n"
          ]
        }
      ],
      "source": [
        "#Confirming Shape\n",
        "Images = test_d\n",
        "Labels = test_l\n",
        "print(Images.shape)\n",
        "print(Labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw2pvdKESk6L"
      },
      "outputs": [],
      "source": [
        "X_data = np.concatenate([test_d,tr_d])\n",
        "X_labels = np.concatenate([test_l,tr_l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgRiUi03W1yU",
        "outputId": "f24662d5-77ce-4386-cc6b-04e1d32f5a82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25172, 224, 224, 3)\n",
            "(25172,)\n"
          ]
        }
      ],
      "source": [
        "#Confirming Shape\n",
        "Images = X_data\n",
        "Labels = X_labels\n",
        "print(Images.shape)\n",
        "print(Labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awbXZ3ucOvdz"
      },
      "source": [
        "# Dividing data using Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ne1OhtTkr1y",
        "outputId": "6ee2703e-68fc-4fc3-a374-f5554ea300f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x train= (20137, 224, 224, 3)\n",
            "y train= (20137,)\n",
            "x test= (5035, 224, 224, 3)\n",
            "y test= (5035,)\n"
          ]
        }
      ],
      "source": [
        "X_Train,X_test,Y_Train,Y_test=train_test_split(Images,Labels,test_size=0.2,random_state = 3, shuffle=True)\n",
        "\n",
        "print(\"x train=\",X_Train.shape)\n",
        "print(\"y train=\",Y_Train.shape)\n",
        "\n",
        "print(\"x test=\",X_test.shape)\n",
        "print(\"y test=\",Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTcfWWC0QLJ2",
        "outputId": "a579ccaa-15a7-4493-f3b3-623d1ce095c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x train= (18123, 224, 224, 3)\n",
            "y train= (18123,)\n",
            "x valid= (2014, 224, 224, 3)\n",
            "y valid= (2014,)\n"
          ]
        }
      ],
      "source": [
        "X_train,X_Val,Y_train,Y_Val=train_test_split(X_Train,Y_Train,test_size=0.1,random_state = 3, shuffle=True)\n",
        "\n",
        "print(\"x train=\",X_train.shape)\n",
        "print(\"y train=\",Y_train.shape)\n",
        "\n",
        "print(\"x valid=\",X_Val.shape)\n",
        "print(\"y valid=\",Y_Val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mChr_bz7djxp"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        #layers.Resizing(image_size, image_size),\n",
        "        # layers.RandomFlip(\"horizontal\"),\n",
        "        # layers.RandomRotation(factor=0.02),\n",
        "        # layers.RandomZoom(\n",
        "        #     height_factor=0.2, width_factor=0.2\n",
        "\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(X_train)\n",
        "data_augmentation.layers[0].adapt(X_Val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD5z17D4ivfo"
      },
      "outputs": [],
      "source": [
        "nClasses=8\n",
        "shape_x = 224\n",
        "shape_y = 224\n",
        "# scale=0.15\n",
        "batch_size =32\n",
        "epochs = 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrMYnwidUJ1N"
      },
      "source": [
        "## Model Architecture\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooaCEtzCJpr2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "def repeat_elem(tensor, rep):\n",
        "    # lambda function to repeat Repeats the elements of a tensor along an axis\n",
        "    #by a factor of rep.\n",
        "    # If tensor has shape (None, 256,256,3), lambda will return a tensor of shape\n",
        "    #(None, 256,256,6), if specified axis=3 and rep=2.\n",
        "\n",
        "     return layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n",
        "                          arguments={'repnum': rep})(tensor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gating_signal(input, out_size, batch_norm=False):\n",
        "    \"\"\"\n",
        "    resize the down layer feature map into the same dimension as the up layer feature map\n",
        "    using 1x1 conv\n",
        "    :return: the gating feature map with the same dimension of the up layer feature map\n",
        "    \"\"\"\n",
        "    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)\n",
        "    if batch_norm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def attention_block(x, gating, inter_shape):\n",
        "    shape_x = K.int_shape(x)\n",
        "    shape_g = K.int_shape(gating)\n",
        "\n",
        "# Getting the x signal to the same shape as the gating signal\n",
        "    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(1, 1), padding='same')(x)  # 16\n",
        "    shape_theta_x = K.int_shape(theta_x)\n",
        "\n",
        "# Getting the gating signal to the same number of filters as the inter_shape\n",
        "    phi_g = layers.Conv2D(inter_shape, (1, 1),  padding='same')(gating)\n",
        "    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3),\n",
        "                                 strides=(1,1),\n",
        "                                 padding='same')(phi_g)  # 16\n",
        "\n",
        "    concat_xg = layers.add([upsample_g, theta_x])\n",
        "    act_xg = layers.Activation('relu')(concat_xg)\n",
        "    psi = layers.Conv2D(1, (1, 1), padding='same')(act_xg)\n",
        "    sigmoid_xg = layers.Activation('sigmoid')(psi)\n",
        "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
        "    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
        "\n",
        "    upsample_psi = repeat_elem(upsample_psi, shape_x[3])\n",
        "\n",
        "    y = layers.multiply([upsample_psi, x])\n",
        "\n",
        "    result = layers.Conv2D(shape_x[3], (1, 1), padding='same')(y)\n",
        "    result_bn = layers.BatchNormalization()(result)\n",
        "    return result_bn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIOaca-Papkj"
      },
      "outputs": [],
      "source": [
        "## AIR-CNN\n",
        "def IncresModel(name='air-cnn'):\n",
        "\n",
        "\n",
        "  #Convolutoinal Layers\n",
        "  # input_img = Input(shape=img_input)\n",
        "\n",
        "  img_input = Input(shape=(shape_x, shape_y, 3))\n",
        "  x = Conv2D(8,(3,3), strides=(2,2),padding='same',data_format='channels_last',use_bias=False,name='conv_input')(img_input)\n",
        "  x = MaxPooling2D((2,2),strides=(2,2),padding='same',name='max_input')(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #inception block A-1\n",
        "\n",
        "  # tower 1\n",
        "  layer_1 = Conv2D(8, (1,1), strides=(1,1), padding='same', activation='LeakyReLU', name='conv_A1')(x)\n",
        "  layer_1 = Conv2D(16, (3,3), strides=(1,1), padding='same',  activation='LeakyReLU',name='conv_A1_1')(layer_1)\n",
        "  layer_1 = Conv2D(16, (3,3), strides=(1,1), padding='same', activation='LeakyReLU',name='conv_A1_11')(layer_1)\n",
        "\n",
        "\n",
        "  # tower 2\n",
        "  layer_2 = Conv2D(8, (1,1), strides=(1,1), padding='same', activation='LeakyReLU',name='conv_A1_2')(x)\n",
        "  layer_2 = Conv2D(16, (3,3), strides=(1,1), padding='same', activation='LeakyReLU',name='conv_A1_22')(layer_2)\n",
        "\n",
        "  # tower 4\n",
        "  layer_3 = Conv2D(16, (1,1), strides=(1,1),padding='same', activation='LeakyReLU',name='conv_A1_3')(x)\n",
        "\n",
        "  inep_block_A1 = concatenate([layer_1,layer_2,layer_3], axis = 3)\n",
        "\n",
        "   ###############################################################################################\n",
        "  inep_block_A1 = concatenate([inep_block_A1,x], axis = 3)\n",
        "\n",
        "   #inception block A-2\n",
        "\n",
        "  # tower 1\n",
        "  layer_1 = Conv2D(8, (1,1), strides=(1,1),  padding='same', activation='LeakyReLU',name='conv_A2')(inep_block_A1)\n",
        "  layer_1 = Conv2D(16, (3,3), strides=(1,1), padding='same',  activation='LeakyReLU', name='conv_A2_1')(layer_1)\n",
        "  layer_1 = Conv2D(16, (3,3), strides=(1,1), padding='same', activation='LeakyReLU' , name='conv_A2_11')(layer_1)\n",
        "\n",
        "\n",
        "  # tower 2\n",
        "  layer_2 = Conv2D(8, (1,1), strides=(1,1), padding='same', activation='LeakyReLU', name='conv_A2_2')(inep_block_A1)\n",
        "  layer_2 = Conv2D(16, (3,3), strides=(1,1), padding='same', activation='LeakyReLU' , name='conv_A2_22')(layer_2)\n",
        "\n",
        "\n",
        "  # tower 4\n",
        "  layer_3 = Conv2D(16, (1,1), strides=(1,1),padding='same', activation='LeakyReLU' , name='conv_A2_3')(inep_block_A1)\n",
        "\n",
        "\n",
        "  inep_block_A2 = concatenate([layer_1,layer_2,layer_3], axis = 3)\n",
        "\n",
        "   ###############################################################################################\n",
        "  inep_block_A2 = concatenate([inep_block_A1,inep_block_A2], axis = 3)\n",
        "\n",
        "   #inception block A-3\n",
        "\n",
        "  # tower 1\n",
        "  layer_1 = Conv2D(16, (1,1), strides=(1,1),  padding='same', activation='LeakyReLU',name='conv_A3')(inep_block_A2)\n",
        "  layer_1 = Conv2D(32, (3,3), strides=(1,1), padding='same',  activation='LeakyReLU',name='conv_A3_1')(layer_1)\n",
        "  layer_1 = Conv2D(32, (3,3), strides=(1,1),  padding='same', activation='LeakyReLU',name='conv_A3_11')(layer_1)\n",
        "\n",
        "\n",
        "  # tower 2\n",
        "  layer_2 = Conv2D(16, (1,1), strides=(1,1), padding='same', activation='LeakyReLU',name='conv_A3_2')(inep_block_A2)\n",
        "  layer_2 = Conv2D(32, (3,3), strides=(1,1), padding='same', activation='LeakyReLU',name='conv_A3_22')(layer_2)\n",
        "\n",
        "\n",
        "  # tower 4\n",
        "  layer_3 = Conv2D(32, (1,1), strides=(1,1),padding='same', activation='LeakyReLU',name='conv_A3_3')(inep_block_A2)\n",
        "\n",
        "\n",
        "  inep_block_A3 = concatenate([layer_1,layer_2,layer_3], axis = 3)\n",
        "\n",
        "   ###############################################################################################\n",
        "  inep_block_A3 = concatenate([inep_block_A3,inep_block_A2], axis = 3)\n",
        "\n",
        "  FILTER_NUM = 16\n",
        "  batch_norm = False\n",
        "  gating_16 = gating_signal(inep_block_A3, 8*FILTER_NUM, batch_norm)\n",
        "  att_16 = attention_block(inep_block_A3, gating_16, 8*FILTER_NUM)\n",
        "\n",
        "  #inception block A-4\n",
        "\n",
        "  # tower 1\n",
        "  layer_1 = Conv2D(16, (1,1), strides=(1,1),  padding='same', activation='LeakyReLU',name='conv_A4')(att_16)\n",
        "  layer_1 = Conv2D(32, (3,3), strides=(1,1), padding='same',  activation='LeakyReLU',name='conv_A4_1')(layer_1)\n",
        "  layer_1 = Conv2D(32, (3,3), strides=(1,1),  padding='same', activation='LeakyReLU',name='conv_A4_11')(layer_1)\n",
        "\n",
        "\n",
        "  # tower 2\n",
        "  layer_2 = Conv2D(16, (1,1), strides=(1,1), padding='same', activation='LeakyReLU',name='conv_A4_2')(att_16)\n",
        "  layer_2 = Conv2D(32, (3,3), strides=(1,1),padding='same', activation='LeakyReLU',name='conv_A4_22')(layer_2)\n",
        "\n",
        "\n",
        "  # tower 4\n",
        "  layer_3 = Conv2D(32, (1,1), strides=(1,1),padding='same', activation='LeakyReLU',name='conv_A4_3')(att_16)\n",
        "\n",
        "\n",
        "  inep_block_A4 = concatenate([layer_1,layer_2,layer_3], axis = 3)\n",
        "\n",
        "   ###############################################################################################\n",
        "#   layer_3 = Conv2D(32, (1,1), strides=(2,2),padding='same', activation='LeakyReLU',name='incep-v3')(inep_block_A3)\n",
        "\n",
        "  inep_block_A4 = concatenate([att_16,inep_block_A4], axis = 3)\n",
        "\n",
        "\n",
        "  #inception block A-5\n",
        "\n",
        "  # tower 1\n",
        "  layer_1 = Conv2D(32, (1,1), strides=(1,1),  padding='same', activation='LeakyReLU',name='conv_A5')(inep_block_A4)\n",
        "  layer_1 = Conv2D(64, (3,3), strides=(1,1), padding='same',  activation='LeakyReLU',name='conv_A5_1')(layer_1)\n",
        "  layer_1 = Conv2D(64, (3,3),strides=(1,1), padding='same', activation='LeakyReLU',name='conv_A5_11')(layer_1)\n",
        "\n",
        "\n",
        "  # tower 2\n",
        "  layer_2 = Conv2D(32, (1,1), strides=(1,1), padding='same', activation='LeakyReLU',name='conv_A5_2')(inep_block_A4)\n",
        "  layer_2 = Conv2D(64, (3,3), strides=(1,1), padding='same', activation='LeakyReLU',name='conv_A5_22')(layer_2)\n",
        "\n",
        "\n",
        "  # tower 4\n",
        "  layer_3 = Conv2D(32, (1,1), strides=(1,1),padding='same', activation='LeakyReLU',name='conv_A5_3')(inep_block_A4)\n",
        "\n",
        "\n",
        "  inep_block_A5 = concatenate([layer_1,layer_2,layer_3], axis = 3)\n",
        "\n",
        "   ###############################################################################################\n",
        "  inep_block_A5 = concatenate([inep_block_A5,inep_block_A4], axis = 3)\n",
        "\n",
        "  #inception block A-6\n",
        "\n",
        "  # tower 1\n",
        "  layer_1 = Conv2D(32, (1,1), strides=(1,1),  padding='same', activation='LeakyReLU',name='conv_A6')(inep_block_A5)\n",
        "  layer_1 = Conv2D(64, (3,3), strides=(1,1), padding='same',  activation='LeakyReLU',name='conv_A6_1')(layer_1)\n",
        "  layer_1 = Conv2D(64, (3,3), strides=(1,1), padding='same', activation='LeakyReLU',name='conv_A6_11')(layer_1)\n",
        "\n",
        "\n",
        "  # tower 2\n",
        "  layer_2 = Conv2D(32, (1,1), strides=(1,1), padding='same', activation='LeakyReLU',name='conv_A6_2')(inep_block_A5)\n",
        "  layer_2 = Conv2D(64, (3,3), strides=(1,1),padding='same', activation='LeakyReLU',name='conv_A6_22')(layer_2)\n",
        "\n",
        "\n",
        "  # tower 4\n",
        "  layer_3 = Conv2D(64, (1,1), strides=(1,1),padding='same', activation='LeakyReLU',name='conv_A6_3')(inep_block_A5)\n",
        "\n",
        "\n",
        "  inep_block_A6 = concatenate([layer_1,layer_2,layer_3], axis = 3)\n",
        "\n",
        "   ###############################################################################################\n",
        "  inep_block_A6 = concatenate([inep_block_A5,inep_block_A6], axis = 3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #inception block A-7\n",
        "\n",
        "  # tower 1\n",
        "  layer_1 = Conv2D(32, (1,1), strides=(1,1),  padding='same', activation='LeakyReLU',name='conv_A7')(inep_block_A6)\n",
        "  layer_1 = Conv2D(64, (3,3), strides=(1,1), padding='same',  activation='LeakyReLU',name='conv_A7_1')(layer_1)\n",
        "  layer_1 = Conv2D(64, (3,3), strides=(1,1), padding='same', activation='LeakyReLU',name='conv_A7_11')(layer_1)\n",
        "\n",
        "\n",
        "  # tower 2\n",
        "  layer_2 = Conv2D(32, (1,1), strides=(1,1), padding='same', activation='LeakyReLU',name='conv_A7_2')(inep_block_A6)\n",
        "  layer_2 = Conv2D(64, (3,3), strides=(1,1),padding='same', activation='LeakyReLU',name='conv_A7_22')(layer_2)\n",
        "\n",
        "\n",
        "  # tower 4\n",
        "  layer_3 = Conv2D(64, (1,1), strides=(1,1),padding='same', activation='LeakyReLU',name='conv_A7_3')(inep_block_A6)\n",
        "\n",
        "\n",
        "  inep_block_A7 = concatenate([layer_1,layer_2,layer_3], axis = 3)\n",
        "\n",
        "   ###############################################################################################\n",
        "  inep_block_A7 = concatenate([inep_block_A7,inep_block_A6], axis = 3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  X = GlobalAveragePooling2D(name = 'GAPL')(inep_block_A7)\n",
        "\n",
        "\n",
        "\n",
        "  dense_1 = Dropout(0.5)(X)\n",
        "  dense_2 = Dense(256, activation='relu')(dense_1)\n",
        "\n",
        "  #dense_3 = Dense(150, activation='relu')(dense_2)\n",
        "  output = Dense(nClasses, activation='softmax')(dense_2)\n",
        "\n",
        "  model = Model([img_input] , output)\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzyKcUwlA8aj",
        "outputId": "228fa63c-7d9e-4fce-9df5-051b0d76fca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_input (Conv2D)             (None, 112, 112, 8)  216         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_input (MaxPooling2D)        (None, 56, 56, 8)    0           conv_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv_A1 (Conv2D)                (None, 56, 56, 8)    72          max_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A1_1 (Conv2D)              (None, 56, 56, 16)   1168        conv_A1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_A1_2 (Conv2D)              (None, 56, 56, 8)    72          max_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A1_11 (Conv2D)             (None, 56, 56, 16)   2320        conv_A1_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A1_22 (Conv2D)             (None, 56, 56, 16)   1168        conv_A1_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A1_3 (Conv2D)              (None, 56, 56, 16)   144         max_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 56, 56, 48)   0           conv_A1_11[0][0]                 \n",
            "                                                                 conv_A1_22[0][0]                 \n",
            "                                                                 conv_A1_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 56, 56, 56)   0           concatenate[0][0]                \n",
            "                                                                 max_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A2 (Conv2D)                (None, 56, 56, 8)    456         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_A2_1 (Conv2D)              (None, 56, 56, 16)   1168        conv_A2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_A2_2 (Conv2D)              (None, 56, 56, 8)    456         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_A2_11 (Conv2D)             (None, 56, 56, 16)   2320        conv_A2_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A2_22 (Conv2D)             (None, 56, 56, 16)   1168        conv_A2_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A2_3 (Conv2D)              (None, 56, 56, 16)   912         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 56, 56, 48)   0           conv_A2_11[0][0]                 \n",
            "                                                                 conv_A2_22[0][0]                 \n",
            "                                                                 conv_A2_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 56, 56, 104)  0           concatenate_1[0][0]              \n",
            "                                                                 concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_A3 (Conv2D)                (None, 56, 56, 16)   1680        concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_A3_1 (Conv2D)              (None, 56, 56, 32)   4640        conv_A3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_A3_2 (Conv2D)              (None, 56, 56, 16)   1680        concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_A3_11 (Conv2D)             (None, 56, 56, 32)   9248        conv_A3_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A3_22 (Conv2D)             (None, 56, 56, 32)   4640        conv_A3_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A3_3 (Conv2D)              (None, 56, 56, 32)   3360        concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 56, 56, 96)   0           conv_A3_11[0][0]                 \n",
            "                                                                 conv_A3_22[0][0]                 \n",
            "                                                                 conv_A3_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 56, 56, 200)  0           concatenate_4[0][0]              \n",
            "                                                                 concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 56, 56, 128)  25728       concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 56, 56, 128)  0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 56, 56, 128)  16512       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose (Conv2DTranspo (None, 56, 56, 128)  147584      conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 56, 56, 128)  102528      concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 56, 56, 128)  0           conv2d_transpose[0][0]           \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 56, 56, 128)  0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 56, 56, 1)    129         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 56, 56, 1)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)    (None, 56, 56, 1)    0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 56, 56, 200)  0           up_sampling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 56, 56, 200)  0           lambda[0][0]                     \n",
            "                                                                 concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 56, 56, 200)  40200       multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 56, 56, 200)  800         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv_A4 (Conv2D)                (None, 56, 56, 16)   3216        batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv_A4_1 (Conv2D)              (None, 56, 56, 32)   4640        conv_A4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_A4_2 (Conv2D)              (None, 56, 56, 16)   3216        batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv_A4_11 (Conv2D)             (None, 56, 56, 32)   9248        conv_A4_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A4_22 (Conv2D)             (None, 56, 56, 32)   4640        conv_A4_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A4_3 (Conv2D)              (None, 56, 56, 32)   6432        batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 56, 56, 96)   0           conv_A4_11[0][0]                 \n",
            "                                                                 conv_A4_22[0][0]                 \n",
            "                                                                 conv_A4_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 56, 56, 296)  0           batch_normalization[0][0]        \n",
            "                                                                 concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_A5 (Conv2D)                (None, 56, 56, 32)   9504        concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_A5_1 (Conv2D)              (None, 56, 56, 64)   18496       conv_A5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_A5_2 (Conv2D)              (None, 56, 56, 32)   9504        concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_A5_11 (Conv2D)             (None, 56, 56, 64)   36928       conv_A5_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A5_22 (Conv2D)             (None, 56, 56, 64)   18496       conv_A5_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A5_3 (Conv2D)              (None, 56, 56, 32)   9504        concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 56, 56, 160)  0           conv_A5_11[0][0]                 \n",
            "                                                                 conv_A5_22[0][0]                 \n",
            "                                                                 conv_A5_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 56, 56, 456)  0           concatenate_8[0][0]              \n",
            "                                                                 concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_A6 (Conv2D)                (None, 56, 56, 32)   14624       concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_A6_1 (Conv2D)              (None, 56, 56, 64)   18496       conv_A6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_A6_2 (Conv2D)              (None, 56, 56, 32)   14624       concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_A6_11 (Conv2D)             (None, 56, 56, 64)   36928       conv_A6_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A6_22 (Conv2D)             (None, 56, 56, 64)   18496       conv_A6_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A6_3 (Conv2D)              (None, 56, 56, 64)   29248       concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 56, 56, 192)  0           conv_A6_11[0][0]                 \n",
            "                                                                 conv_A6_22[0][0]                 \n",
            "                                                                 conv_A6_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 56, 56, 648)  0           concatenate_9[0][0]              \n",
            "                                                                 concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_A7 (Conv2D)                (None, 56, 56, 32)   20768       concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_A7_1 (Conv2D)              (None, 56, 56, 64)   18496       conv_A7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_A7_2 (Conv2D)              (None, 56, 56, 32)   20768       concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_A7_11 (Conv2D)             (None, 56, 56, 64)   36928       conv_A7_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A7_22 (Conv2D)             (None, 56, 56, 64)   18496       conv_A7_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv_A7_3 (Conv2D)              (None, 56, 56, 64)   41536       concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 56, 56, 192)  0           conv_A7_11[0][0]                 \n",
            "                                                                 conv_A7_22[0][0]                 \n",
            "                                                                 conv_A7_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 56, 56, 840)  0           concatenate_12[0][0]             \n",
            "                                                                 concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "GAPL (GlobalAveragePooling2D)   (None, 840)          0           concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 840)          0           GAPL[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          215296      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 8)            2056        dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,010,953\n",
            "Trainable params: 1,010,553\n",
            "Non-trainable params: 400\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#Input Layer\n",
        "\n",
        "model = IncresModel(name='air-cnn')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nYIKLZYuZeWL",
        "outputId": "7e099734-78a7-4d3e-9ce6-573f3667b120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model,to_file=\"model.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQG-o4E9IRrR"
      },
      "source": [
        "## Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms6ghBctEksk"
      },
      "outputs": [],
      "source": [
        "##compile the model.\n",
        "import tensorflow\n",
        "learning_rate = 0.0001\n",
        "optimizer = tensorflow.keras.optimizers.Adam(learning_rate)\n",
        "model.compile(optimizer = optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDe11QOHE1Y6",
        "outputId": "a5edbc21-c657-49df-a6ab-bd2b39f02dd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\")\n"
          ]
        }
      ],
      "source": [
        "print(model.input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1MspzJE1Y7",
        "outputId": "788259b1-04b9-44d1-958b-80ca5d6ed654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='dense_1/Softmax:0', description=\"created by layer 'dense_1'\")\n"
          ]
        }
      ],
      "source": [
        "print(model.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrsMo3kGId_a"
      },
      "source": [
        "## Start Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzyCMDA_TcH5"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = 'D:/SaraYounas/SavedModels'\n",
        "callbacks = []\n",
        "callbacks.append(ModelCheckpoint(checkpoint_path + '/AIR-CNN-model.h5'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FhAhQBBFg-u",
        "outputId": "8a8cdbeb-052b-4a4f-9154-27f09674b1e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "567/567 [==============================] - 178s 293ms/step - loss: 1.7994 - accuracy: 0.3157 - val_loss: 1.8215 - val_accuracy: 0.3044\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HP8\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  warnings.warn('Custom mask layers require a config and must override '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/500\n",
            "567/567 [==============================] - 146s 258ms/step - loss: 1.5441 - accuracy: 0.4023 - val_loss: 1.8116 - val_accuracy: 0.3242\n",
            "Epoch 3/500\n",
            "567/567 [==============================] - 127s 224ms/step - loss: 1.4623 - accuracy: 0.4319 - val_loss: 1.9218 - val_accuracy: 0.2895\n",
            "Epoch 4/500\n",
            "567/567 [==============================] - 121s 213ms/step - loss: 1.4020 - accuracy: 0.4515 - val_loss: 1.4740 - val_accuracy: 0.4101\n",
            "Epoch 5/500\n",
            "567/567 [==============================] - 119s 210ms/step - loss: 1.3635 - accuracy: 0.4668 - val_loss: 1.3538 - val_accuracy: 0.4657\n",
            "Epoch 6/500\n",
            "567/567 [==============================] - 119s 210ms/step - loss: 1.3347 - accuracy: 0.4801 - val_loss: 1.3845 - val_accuracy: 0.4598\n",
            "Epoch 7/500\n",
            "567/567 [==============================] - 119s 209ms/step - loss: 1.2960 - accuracy: 0.4910 - val_loss: 1.6861 - val_accuracy: 0.3952\n",
            "Epoch 8/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 1.2746 - accuracy: 0.5051 - val_loss: 1.3667 - val_accuracy: 0.4752\n",
            "Epoch 9/500\n",
            "567/567 [==============================] - 118s 207ms/step - loss: 1.2482 - accuracy: 0.5105 - val_loss: 1.3093 - val_accuracy: 0.5079\n",
            "Epoch 10/500\n",
            "567/567 [==============================] - 118s 207ms/step - loss: 1.2306 - accuracy: 0.5220 - val_loss: 1.9706 - val_accuracy: 0.3491\n",
            "Epoch 11/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 1.2015 - accuracy: 0.5320 - val_loss: 1.4466 - val_accuracy: 0.4573\n",
            "Epoch 12/500\n",
            "567/567 [==============================] - 114s 201ms/step - loss: 1.1907 - accuracy: 0.5321 - val_loss: 1.2552 - val_accuracy: 0.5114\n",
            "Epoch 13/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 1.1738 - accuracy: 0.5444 - val_loss: 1.2058 - val_accuracy: 0.5362\n",
            "Epoch 14/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 1.1460 - accuracy: 0.5536 - val_loss: 1.3147 - val_accuracy: 0.5089\n",
            "Epoch 15/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 1.1244 - accuracy: 0.5660 - val_loss: 1.4435 - val_accuracy: 0.5000\n",
            "Epoch 16/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 1.1031 - accuracy: 0.5726 - val_loss: 1.1450 - val_accuracy: 0.5551\n",
            "Epoch 17/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 1.0875 - accuracy: 0.5801 - val_loss: 1.1985 - val_accuracy: 0.5407\n",
            "Epoch 18/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 1.0757 - accuracy: 0.5815 - val_loss: 1.2264 - val_accuracy: 0.5412\n",
            "Epoch 19/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 1.0584 - accuracy: 0.5884 - val_loss: 1.4019 - val_accuracy: 0.5094\n",
            "Epoch 20/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 1.0455 - accuracy: 0.5957 - val_loss: 1.3320 - val_accuracy: 0.5035\n",
            "Epoch 21/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 1.0304 - accuracy: 0.6009 - val_loss: 1.0297 - val_accuracy: 0.5983\n",
            "Epoch 22/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 1.0141 - accuracy: 0.6094 - val_loss: 1.0805 - val_accuracy: 0.5834\n",
            "Epoch 23/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 1.0030 - accuracy: 0.6123 - val_loss: 1.7496 - val_accuracy: 0.4816\n",
            "Epoch 24/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.9872 - accuracy: 0.6160 - val_loss: 0.9810 - val_accuracy: 0.6370\n",
            "Epoch 25/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.9801 - accuracy: 0.6198 - val_loss: 1.2745 - val_accuracy: 0.5338\n",
            "Epoch 26/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.9719 - accuracy: 0.6266 - val_loss: 2.4208 - val_accuracy: 0.4096\n",
            "Epoch 27/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.9594 - accuracy: 0.6290 - val_loss: 1.2408 - val_accuracy: 0.5521\n",
            "Epoch 28/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.9442 - accuracy: 0.6332 - val_loss: 1.0938 - val_accuracy: 0.5889\n",
            "Epoch 29/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.9347 - accuracy: 0.6380 - val_loss: 1.3321 - val_accuracy: 0.5298\n",
            "Epoch 30/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.9265 - accuracy: 0.6379 - val_loss: 1.1950 - val_accuracy: 0.5720\n",
            "Epoch 31/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.9112 - accuracy: 0.6469 - val_loss: 1.2505 - val_accuracy: 0.5561\n",
            "Epoch 32/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.9079 - accuracy: 0.6479 - val_loss: 0.9309 - val_accuracy: 0.6490\n",
            "Epoch 33/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.8903 - accuracy: 0.6570 - val_loss: 1.0009 - val_accuracy: 0.6187\n",
            "Epoch 34/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.8857 - accuracy: 0.6567 - val_loss: 1.0112 - val_accuracy: 0.6077\n",
            "Epoch 35/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.8792 - accuracy: 0.6609 - val_loss: 1.1619 - val_accuracy: 0.5611\n",
            "Epoch 36/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.8676 - accuracy: 0.6649 - val_loss: 1.1022 - val_accuracy: 0.5914\n",
            "Epoch 37/500\n",
            "567/567 [==============================] - 115s 202ms/step - loss: 0.8540 - accuracy: 0.6690 - val_loss: 1.8114 - val_accuracy: 0.4707\n",
            "Epoch 38/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.8526 - accuracy: 0.6752 - val_loss: 1.1169 - val_accuracy: 0.5695\n",
            "Epoch 39/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.8422 - accuracy: 0.6758 - val_loss: 1.0073 - val_accuracy: 0.6212\n",
            "Epoch 40/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.8456 - accuracy: 0.6721 - val_loss: 1.9695 - val_accuracy: 0.4742\n",
            "Epoch 41/500\n",
            "567/567 [==============================] - 115s 204ms/step - loss: 0.8315 - accuracy: 0.6779 - val_loss: 1.0587 - val_accuracy: 0.6212\n",
            "Epoch 42/500\n",
            "567/567 [==============================] - 114s 201ms/step - loss: 0.8219 - accuracy: 0.6815 - val_loss: 0.9323 - val_accuracy: 0.6261\n",
            "Epoch 43/500\n",
            "567/567 [==============================] - 115s 202ms/step - loss: 0.8165 - accuracy: 0.6838 - val_loss: 0.9589 - val_accuracy: 0.6465\n",
            "Epoch 44/500\n",
            "567/567 [==============================] - 115s 204ms/step - loss: 0.8096 - accuracy: 0.6897 - val_loss: 0.8994 - val_accuracy: 0.6639\n",
            "Epoch 45/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.8027 - accuracy: 0.6895 - val_loss: 0.9053 - val_accuracy: 0.6460\n",
            "Epoch 46/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.7974 - accuracy: 0.6915 - val_loss: 1.5346 - val_accuracy: 0.5348\n",
            "Epoch 47/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.7902 - accuracy: 0.6946 - val_loss: 0.8693 - val_accuracy: 0.6683\n",
            "Epoch 48/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.7815 - accuracy: 0.6992 - val_loss: 0.9310 - val_accuracy: 0.6435\n",
            "Epoch 49/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.7682 - accuracy: 0.7029 - val_loss: 0.8356 - val_accuracy: 0.6812\n",
            "Epoch 50/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.7672 - accuracy: 0.7021 - val_loss: 0.8440 - val_accuracy: 0.6867\n",
            "Epoch 51/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.7637 - accuracy: 0.7073 - val_loss: 0.9365 - val_accuracy: 0.6430\n",
            "Epoch 52/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.7535 - accuracy: 0.7120 - val_loss: 0.9844 - val_accuracy: 0.6306\n",
            "Epoch 53/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.7489 - accuracy: 0.7109 - val_loss: 0.9571 - val_accuracy: 0.6470\n",
            "Epoch 54/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.7388 - accuracy: 0.7180 - val_loss: 0.8300 - val_accuracy: 0.6991\n",
            "Epoch 55/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.7352 - accuracy: 0.7149 - val_loss: 0.8208 - val_accuracy: 0.6783\n",
            "Epoch 56/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.7271 - accuracy: 0.7190 - val_loss: 0.9313 - val_accuracy: 0.6683\n",
            "Epoch 57/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.7134 - accuracy: 0.7270 - val_loss: 2.6780 - val_accuracy: 0.4538\n",
            "Epoch 58/500\n",
            "567/567 [==============================] - 118s 207ms/step - loss: 0.7219 - accuracy: 0.7242 - val_loss: 1.2239 - val_accuracy: 0.5859\n",
            "Epoch 59/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.7079 - accuracy: 0.7287 - val_loss: 0.7939 - val_accuracy: 0.7036\n",
            "Epoch 60/500\n",
            "567/567 [==============================] - 118s 207ms/step - loss: 0.7096 - accuracy: 0.7285 - val_loss: 1.0745 - val_accuracy: 0.6167\n",
            "Epoch 61/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.7000 - accuracy: 0.7315 - val_loss: 0.9637 - val_accuracy: 0.6584\n",
            "Epoch 62/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.6923 - accuracy: 0.7351 - val_loss: 0.7581 - val_accuracy: 0.7115\n",
            "Epoch 63/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.6940 - accuracy: 0.7368 - val_loss: 1.0596 - val_accuracy: 0.6207\n",
            "Epoch 64/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.6802 - accuracy: 0.7353 - val_loss: 0.8447 - val_accuracy: 0.6812\n",
            "Epoch 65/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.6805 - accuracy: 0.7386 - val_loss: 0.8889 - val_accuracy: 0.6663\n",
            "Epoch 66/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.6714 - accuracy: 0.7417 - val_loss: 1.0909 - val_accuracy: 0.6261\n",
            "Epoch 67/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.6670 - accuracy: 0.7457 - val_loss: 0.8629 - val_accuracy: 0.6872\n",
            "Epoch 68/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.6642 - accuracy: 0.7438 - val_loss: 0.8079 - val_accuracy: 0.6941\n",
            "Epoch 69/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.6563 - accuracy: 0.7479 - val_loss: 0.7881 - val_accuracy: 0.7140\n",
            "Epoch 70/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.6492 - accuracy: 0.7484 - val_loss: 0.8486 - val_accuracy: 0.6941\n",
            "Epoch 71/500\n",
            "567/567 [==============================] - 118s 209ms/step - loss: 0.6527 - accuracy: 0.7512 - val_loss: 1.0033 - val_accuracy: 0.6529\n",
            "Epoch 72/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.6454 - accuracy: 0.7516 - val_loss: 0.7756 - val_accuracy: 0.7190\n",
            "Epoch 73/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.6370 - accuracy: 0.7597 - val_loss: 0.7501 - val_accuracy: 0.7125\n",
            "Epoch 74/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.6274 - accuracy: 0.7593 - val_loss: 0.6744 - val_accuracy: 0.7493\n",
            "Epoch 75/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.6342 - accuracy: 0.7540 - val_loss: 0.7254 - val_accuracy: 0.7299\n",
            "Epoch 76/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.6245 - accuracy: 0.7616 - val_loss: 0.8486 - val_accuracy: 0.6946\n",
            "Epoch 77/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.6091 - accuracy: 0.7661 - val_loss: 0.7898 - val_accuracy: 0.7100\n",
            "Epoch 78/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.6134 - accuracy: 0.7598 - val_loss: 0.8995 - val_accuracy: 0.6797\n",
            "Epoch 79/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.5970 - accuracy: 0.7691 - val_loss: 0.6527 - val_accuracy: 0.7612\n",
            "Epoch 80/500\n",
            "567/567 [==============================] - 118s 207ms/step - loss: 0.6066 - accuracy: 0.7675 - val_loss: 0.7672 - val_accuracy: 0.7115\n",
            "Epoch 81/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.5882 - accuracy: 0.7766 - val_loss: 0.8508 - val_accuracy: 0.6912\n",
            "Epoch 82/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.5886 - accuracy: 0.7749 - val_loss: 0.7562 - val_accuracy: 0.7175\n",
            "Epoch 83/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.5861 - accuracy: 0.7756 - val_loss: 0.8223 - val_accuracy: 0.6986\n",
            "Epoch 84/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.5867 - accuracy: 0.7755 - val_loss: 0.8749 - val_accuracy: 0.6857\n",
            "Epoch 85/500\n",
            "567/567 [==============================] - 118s 207ms/step - loss: 0.5829 - accuracy: 0.7796 - val_loss: 1.1116 - val_accuracy: 0.6509\n",
            "Epoch 86/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.5660 - accuracy: 0.7861 - val_loss: 0.6323 - val_accuracy: 0.7517\n",
            "Epoch 87/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.5712 - accuracy: 0.7804 - val_loss: 0.6677 - val_accuracy: 0.7676\n",
            "Epoch 88/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.5604 - accuracy: 0.7841 - val_loss: 1.0435 - val_accuracy: 0.6360\n",
            "Epoch 89/500\n",
            "567/567 [==============================] - 115s 204ms/step - loss: 0.5554 - accuracy: 0.7886 - val_loss: 1.2436 - val_accuracy: 0.6256\n",
            "Epoch 90/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.5543 - accuracy: 0.7877 - val_loss: 0.8108 - val_accuracy: 0.7036\n",
            "Epoch 91/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.5570 - accuracy: 0.7870 - val_loss: 0.8858 - val_accuracy: 0.6912\n",
            "Epoch 92/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.5504 - accuracy: 0.7888 - val_loss: 0.9308 - val_accuracy: 0.6703\n",
            "Epoch 93/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.5419 - accuracy: 0.7915 - val_loss: 0.9261 - val_accuracy: 0.6842\n",
            "Epoch 94/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.5459 - accuracy: 0.7919 - val_loss: 0.7916 - val_accuracy: 0.7175\n",
            "Epoch 95/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.5398 - accuracy: 0.7946 - val_loss: 0.9082 - val_accuracy: 0.6683\n",
            "Epoch 96/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.5348 - accuracy: 0.7983 - val_loss: 0.7650 - val_accuracy: 0.7195\n",
            "Epoch 97/500\n",
            "567/567 [==============================] - 115s 204ms/step - loss: 0.5264 - accuracy: 0.7999 - val_loss: 0.9239 - val_accuracy: 0.6713\n",
            "Epoch 98/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.5237 - accuracy: 0.7983 - val_loss: 0.7035 - val_accuracy: 0.7393\n",
            "Epoch 99/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.5299 - accuracy: 0.7991 - val_loss: 0.8865 - val_accuracy: 0.6981\n",
            "Epoch 100/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.5119 - accuracy: 0.8040 - val_loss: 0.7474 - val_accuracy: 0.7418\n",
            "Epoch 101/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.5171 - accuracy: 0.8073 - val_loss: 0.6547 - val_accuracy: 0.7607\n",
            "Epoch 102/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.5124 - accuracy: 0.8049 - val_loss: 0.6760 - val_accuracy: 0.7542\n",
            "Epoch 103/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.5000 - accuracy: 0.8073 - val_loss: 1.0220 - val_accuracy: 0.6693\n",
            "Epoch 104/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.5061 - accuracy: 0.8089 - val_loss: 0.9852 - val_accuracy: 0.6643\n",
            "Epoch 105/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.5016 - accuracy: 0.8061 - val_loss: 0.6060 - val_accuracy: 0.7865\n",
            "Epoch 106/500\n",
            "567/567 [==============================] - 119s 209ms/step - loss: 0.4921 - accuracy: 0.8148 - val_loss: 0.5898 - val_accuracy: 0.7895\n",
            "Epoch 107/500\n",
            "567/567 [==============================] - 118s 209ms/step - loss: 0.4896 - accuracy: 0.8128 - val_loss: 1.0104 - val_accuracy: 0.6872\n",
            "Epoch 108/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.4820 - accuracy: 0.8166 - val_loss: 0.6825 - val_accuracy: 0.7557\n",
            "Epoch 109/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.4861 - accuracy: 0.8167 - val_loss: 0.8510 - val_accuracy: 0.6971\n",
            "Epoch 110/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.4850 - accuracy: 0.8151 - val_loss: 0.5947 - val_accuracy: 0.7870\n",
            "Epoch 111/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.4741 - accuracy: 0.8208 - val_loss: 1.0843 - val_accuracy: 0.6733\n",
            "Epoch 112/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.4740 - accuracy: 0.8203 - val_loss: 0.5699 - val_accuracy: 0.7850curacy\n",
            "Epoch 113/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.4684 - accuracy: 0.8220 - val_loss: 0.9499 - val_accuracy: 0.6822\n",
            "Epoch 114/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.4780 - accuracy: 0.8202 - val_loss: 0.6365 - val_accuracy: 0.7731\n",
            "Epoch 115/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.4655 - accuracy: 0.8233 - val_loss: 0.9060 - val_accuracy: 0.6981\n",
            "Epoch 116/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.4568 - accuracy: 0.8272 - val_loss: 1.5171 - val_accuracy: 0.6162\n",
            "Epoch 117/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.4569 - accuracy: 0.8255 - val_loss: 0.7924 - val_accuracy: 0.7294\n",
            "Epoch 118/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.4444 - accuracy: 0.8301 - val_loss: 0.6473 - val_accuracy: 0.7676\n",
            "Epoch 119/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.4529 - accuracy: 0.8288 - val_loss: 0.7500 - val_accuracy: 0.7373\n",
            "Epoch 120/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.4513 - accuracy: 0.8293 - val_loss: 0.7420 - val_accuracy: 0.7448\n",
            "Epoch 121/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.4449 - accuracy: 0.8338 - val_loss: 0.6374 - val_accuracy: 0.7587\n",
            "Epoch 122/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.4478 - accuracy: 0.8302 - val_loss: 0.5587 - val_accuracy: 0.7964\n",
            "Epoch 123/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.4384 - accuracy: 0.8319 - val_loss: 0.8086 - val_accuracy: 0.7150\n",
            "Epoch 124/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.4373 - accuracy: 0.8333 - val_loss: 0.8478 - val_accuracy: 0.7264\n",
            "Epoch 125/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.4324 - accuracy: 0.8348 - val_loss: 0.7643 - val_accuracy: 0.7473\n",
            "Epoch 126/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.4305 - accuracy: 0.8356 - val_loss: 0.6579 - val_accuracy: 0.7701\n",
            "Epoch 127/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.4308 - accuracy: 0.8374 - val_loss: 0.7934 - val_accuracy: 0.7254\n",
            "Epoch 128/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.4169 - accuracy: 0.8429 - val_loss: 0.6783 - val_accuracy: 0.7522\n",
            "Epoch 129/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.4182 - accuracy: 0.8430 - val_loss: 0.6189 - val_accuracy: 0.7696\n",
            "Epoch 130/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.4278 - accuracy: 0.8382 - val_loss: 0.8013 - val_accuracy: 0.7264\n",
            "Epoch 131/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.4075 - accuracy: 0.8481 - val_loss: 1.3630 - val_accuracy: 0.6405\n",
            "Epoch 132/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.4156 - accuracy: 0.8454 - val_loss: 0.7562 - val_accuracy: 0.7622\n",
            "Epoch 133/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.4055 - accuracy: 0.8469 - val_loss: 0.6742 - val_accuracy: 0.7681\n",
            "Epoch 134/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.4060 - accuracy: 0.8462 - val_loss: 0.5555 - val_accuracy: 0.8054\n",
            "Epoch 135/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.4030 - accuracy: 0.8497 - val_loss: 2.0845 - val_accuracy: 0.5511\n",
            "Epoch 136/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.4024 - accuracy: 0.8491 - val_loss: 0.5972 - val_accuracy: 0.7905\n",
            "Epoch 137/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.3910 - accuracy: 0.8523 - val_loss: 0.7520 - val_accuracy: 0.7776\n",
            "Epoch 138/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.3923 - accuracy: 0.8495 - val_loss: 0.7817 - val_accuracy: 0.7378\n",
            "Epoch 139/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.3885 - accuracy: 0.8492 - val_loss: 0.5464 - val_accuracy: 0.8138\n",
            "Epoch 140/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.3912 - accuracy: 0.8504 - val_loss: 1.0317 - val_accuracy: 0.7066\n",
            "Epoch 141/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.3839 - accuracy: 0.8554 - val_loss: 0.9650 - val_accuracy: 0.6917\n",
            "Epoch 142/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.3780 - accuracy: 0.8583 - val_loss: 0.5379 - val_accuracy: 0.8138\n",
            "Epoch 143/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.3861 - accuracy: 0.8552 - val_loss: 5.9210 - val_accuracy: 0.3878\n",
            "Epoch 144/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.3824 - accuracy: 0.8577 - val_loss: 1.5348 - val_accuracy: 0.6311\n",
            "Epoch 145/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.3701 - accuracy: 0.8594 - val_loss: 0.6415 - val_accuracy: 0.7895\n",
            "Epoch 146/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.3712 - accuracy: 0.8628 - val_loss: 0.6008 - val_accuracy: 0.7840\n",
            "Epoch 147/500\n",
            "567/567 [==============================] - 115s 202ms/step - loss: 0.3767 - accuracy: 0.8598 - val_loss: 0.7437 - val_accuracy: 0.7676\n",
            "Epoch 148/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.3708 - accuracy: 0.8615 - val_loss: 0.6379 - val_accuracy: 0.8014\n",
            "Epoch 149/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.3592 - accuracy: 0.8655 - val_loss: 0.5211 - val_accuracy: 0.8123\n",
            "Epoch 150/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.3579 - accuracy: 0.8676 - val_loss: 0.6795 - val_accuracy: 0.7686\n",
            "Epoch 151/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.3718 - accuracy: 0.8618 - val_loss: 0.7281 - val_accuracy: 0.7651\n",
            "Epoch 152/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.3537 - accuracy: 0.8653 - val_loss: 0.6129 - val_accuracy: 0.7949\n",
            "Epoch 153/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.3522 - accuracy: 0.8654 - val_loss: 0.4484 - val_accuracy: 0.8530\n",
            "Epoch 154/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.3565 - accuracy: 0.8676 - val_loss: 0.4968 - val_accuracy: 0.8307\n",
            "Epoch 155/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.3520 - accuracy: 0.8683 - val_loss: 1.1336 - val_accuracy: 0.6927\n",
            "Epoch 156/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.3448 - accuracy: 0.8698 - val_loss: 0.6142 - val_accuracy: 0.7890\n",
            "Epoch 157/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.3525 - accuracy: 0.8676 - val_loss: 0.5840 - val_accuracy: 0.7939\n",
            "Epoch 158/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.3439 - accuracy: 0.8702 - val_loss: 1.3221 - val_accuracy: 0.6822\n",
            "Epoch 159/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.3355 - accuracy: 0.8753 - val_loss: 0.7549 - val_accuracy: 0.7612\n",
            "Epoch 160/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.3406 - accuracy: 0.8725 - val_loss: 0.5707 - val_accuracy: 0.8034\n",
            "Epoch 161/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.3298 - accuracy: 0.8754 - val_loss: 0.5218 - val_accuracy: 0.8262\n",
            "Epoch 162/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.3451 - accuracy: 0.8697 - val_loss: 0.6762 - val_accuracy: 0.7825\n",
            "Epoch 163/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.3289 - accuracy: 0.8746 - val_loss: 1.1770 - val_accuracy: 0.7046\n",
            "Epoch 164/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.3485 - accuracy: 0.8676 - val_loss: 0.5812 - val_accuracy: 0.8064\n",
            "Epoch 165/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.3226 - accuracy: 0.8795 - val_loss: 0.5546 - val_accuracy: 0.8222\n",
            "Epoch 166/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.3299 - accuracy: 0.8755 - val_loss: 0.9367 - val_accuracy: 0.7264\n",
            "Epoch 167/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.3196 - accuracy: 0.8807 - val_loss: 0.8348 - val_accuracy: 0.7483\n",
            "Epoch 168/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.3259 - accuracy: 0.8770 - val_loss: 0.5381 - val_accuracy: 0.8049\n",
            "Epoch 169/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.3177 - accuracy: 0.8814 - val_loss: 1.4477 - val_accuracy: 0.6643\n",
            "Epoch 170/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.3145 - accuracy: 0.8846 - val_loss: 0.6212 - val_accuracy: 0.7944\n",
            "Epoch 171/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.3184 - accuracy: 0.8798 - val_loss: 0.6261 - val_accuracy: 0.7860\n",
            "Epoch 172/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.3068 - accuracy: 0.8858 - val_loss: 0.8249 - val_accuracy: 0.7517\n",
            "Epoch 173/500\n",
            "567/567 [==============================] - 114s 202ms/step - loss: 0.3111 - accuracy: 0.8837 - val_loss: 0.5022 - val_accuracy: 0.8411\n",
            "Epoch 174/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.3214 - accuracy: 0.8813 - val_loss: 0.5484 - val_accuracy: 0.8088\n",
            "Epoch 175/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.3099 - accuracy: 0.8836 - val_loss: 0.6251 - val_accuracy: 0.8088\n",
            "Epoch 176/500\n",
            "567/567 [==============================] - 115s 204ms/step - loss: 0.3147 - accuracy: 0.8831 - val_loss: 1.0651 - val_accuracy: 0.7006\n",
            "Epoch 177/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.3057 - accuracy: 0.8849 - val_loss: 0.4860 - val_accuracy: 0.8327\n",
            "Epoch 178/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.3051 - accuracy: 0.8857 - val_loss: 0.5945 - val_accuracy: 0.8073\n",
            "Epoch 179/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.3055 - accuracy: 0.8872 - val_loss: 0.7524 - val_accuracy: 0.7547\n",
            "Epoch 180/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.3090 - accuracy: 0.8822 - val_loss: 0.4479 - val_accuracy: 0.8530\n",
            "Epoch 181/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2994 - accuracy: 0.8879 - val_loss: 1.7571 - val_accuracy: 0.6058\n",
            "Epoch 182/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2913 - accuracy: 0.8884 - val_loss: 0.9570 - val_accuracy: 0.7383\n",
            "Epoch 183/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.3020 - accuracy: 0.8877 - val_loss: 1.0544 - val_accuracy: 0.7036\n",
            "Epoch 184/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2863 - accuracy: 0.8930 - val_loss: 0.7512 - val_accuracy: 0.7632\n",
            "Epoch 185/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2904 - accuracy: 0.8928 - val_loss: 0.4591 - val_accuracy: 0.8505\n",
            "Epoch 186/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2946 - accuracy: 0.8909 - val_loss: 0.5545 - val_accuracy: 0.8203\n",
            "Epoch 187/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.2920 - accuracy: 0.8919 - val_loss: 0.6610 - val_accuracy: 0.8029\n",
            "Epoch 188/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.2886 - accuracy: 0.8917 - val_loss: 1.2411 - val_accuracy: 0.6797\n",
            "Epoch 189/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2830 - accuracy: 0.8965 - val_loss: 0.4180 - val_accuracy: 0.8615\n",
            "Epoch 190/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2777 - accuracy: 0.8979 - val_loss: 0.7958 - val_accuracy: 0.7542\n",
            "Epoch 191/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2873 - accuracy: 0.8922 - val_loss: 0.6257 - val_accuracy: 0.7890\n",
            "Epoch 192/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.2969 - accuracy: 0.8879 - val_loss: 1.6249 - val_accuracy: 0.6221\n",
            "Epoch 193/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.2747 - accuracy: 0.8989 - val_loss: 0.4419 - val_accuracy: 0.8451\n",
            "Epoch 194/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2765 - accuracy: 0.8985 - val_loss: 0.7530 - val_accuracy: 0.7825\n",
            "Epoch 195/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.2723 - accuracy: 0.8970 - val_loss: 0.4782 - val_accuracy: 0.8520\n",
            "Epoch 196/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2795 - accuracy: 0.8966 - val_loss: 2.4061 - val_accuracy: 0.5785\n",
            "Epoch 197/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2743 - accuracy: 0.8978 - val_loss: 0.9743 - val_accuracy: 0.7413\n",
            "Epoch 198/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2684 - accuracy: 0.9003 - val_loss: 0.4879 - val_accuracy: 0.8401\n",
            "Epoch 199/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.2667 - accuracy: 0.9005 - val_loss: 0.4267 - val_accuracy: 0.8481\n",
            "Epoch 200/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.2611 - accuracy: 0.9052 - val_loss: 0.9507 - val_accuracy: 0.7339\n",
            "Epoch 201/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2768 - accuracy: 0.8969 - val_loss: 0.6210 - val_accuracy: 0.7934\n",
            "Epoch 202/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.2651 - accuracy: 0.9011 - val_loss: 0.5480 - val_accuracy: 0.8317\n",
            "Epoch 203/500\n",
            "567/567 [==============================] - 115s 202ms/step - loss: 0.2670 - accuracy: 0.8997 - val_loss: 0.5338 - val_accuracy: 0.8312\n",
            "Epoch 204/500\n",
            "567/567 [==============================] - 115s 202ms/step - loss: 0.2626 - accuracy: 0.9023 - val_loss: 0.5255 - val_accuracy: 0.8237\n",
            "Epoch 205/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2657 - accuracy: 0.9017 - val_loss: 0.6577 - val_accuracy: 0.7934\n",
            "Epoch 206/500\n",
            "567/567 [==============================] - 115s 202ms/step - loss: 0.2542 - accuracy: 0.9050 - val_loss: 0.6751 - val_accuracy: 0.8059\n",
            "Epoch 207/500\n",
            "567/567 [==============================] - 114s 202ms/step - loss: 0.2654 - accuracy: 0.9003 - val_loss: 0.6081 - val_accuracy: 0.8237\n",
            "Epoch 208/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2563 - accuracy: 0.9040 - val_loss: 0.4407 - val_accuracy: 0.8555\n",
            "Epoch 209/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2628 - accuracy: 0.9031 - val_loss: 0.4567 - val_accuracy: 0.8510\n",
            "Epoch 210/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.2616 - accuracy: 0.9023 - val_loss: 1.2732 - val_accuracy: 0.6807\n",
            "Epoch 211/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2463 - accuracy: 0.9094 - val_loss: 0.7621 - val_accuracy: 0.7582\n",
            "Epoch 212/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2632 - accuracy: 0.9016 - val_loss: 3.6780 - val_accuracy: 0.4960\n",
            "Epoch 213/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.2463 - accuracy: 0.9061 - val_loss: 0.9933 - val_accuracy: 0.7219\n",
            "Epoch 214/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.2486 - accuracy: 0.9073 - val_loss: 0.3933 - val_accuracy: 0.8669\n",
            "Epoch 215/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.2470 - accuracy: 0.9056 - val_loss: 0.5402 - val_accuracy: 0.8252\n",
            "Epoch 216/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2473 - accuracy: 0.9064 - val_loss: 0.5816 - val_accuracy: 0.8098\n",
            "Epoch 217/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.2568 - accuracy: 0.9060 - val_loss: 0.7839 - val_accuracy: 0.7676\n",
            "Epoch 218/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.2467 - accuracy: 0.9087 - val_loss: 1.2309 - val_accuracy: 0.7006\n",
            "Epoch 219/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.2341 - accuracy: 0.9106 - val_loss: 0.4401 - val_accuracy: 0.8550\n",
            "Epoch 220/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2384 - accuracy: 0.9105 - val_loss: 0.4038 - val_accuracy: 0.8635\n",
            "Epoch 221/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.2480 - accuracy: 0.9092 - val_loss: 1.3887 - val_accuracy: 0.6827\n",
            "Epoch 222/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.2458 - accuracy: 0.9075 - val_loss: 0.4639 - val_accuracy: 0.8347\n",
            "Epoch 223/500\n",
            "567/567 [==============================] - 114s 201ms/step - loss: 0.2424 - accuracy: 0.9108 - val_loss: 0.5506 - val_accuracy: 0.8217\n",
            "Epoch 224/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.2334 - accuracy: 0.9138 - val_loss: 1.7776 - val_accuracy: 0.6331\n",
            "Epoch 225/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2307 - accuracy: 0.9145 - val_loss: 0.5155 - val_accuracy: 0.8332\n",
            "Epoch 226/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.2355 - accuracy: 0.9135 - val_loss: 0.7738 - val_accuracy: 0.7637\n",
            "Epoch 227/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2290 - accuracy: 0.9151 - val_loss: 0.4376 - val_accuracy: 0.8600\n",
            "Epoch 228/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2326 - accuracy: 0.9167 - val_loss: 0.6479 - val_accuracy: 0.8103\n",
            "Epoch 229/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2277 - accuracy: 0.9171 - val_loss: 0.5508 - val_accuracy: 0.8188\n",
            "Epoch 230/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2299 - accuracy: 0.9129 - val_loss: 0.4304 - val_accuracy: 0.8520\n",
            "Epoch 231/500\n",
            "567/567 [==============================] - 115s 204ms/step - loss: 0.2326 - accuracy: 0.9143 - val_loss: 0.5172 - val_accuracy: 0.8535\n",
            "Epoch 232/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.2438 - accuracy: 0.9131 - val_loss: 1.4765 - val_accuracy: 0.6639\n",
            "Epoch 233/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.2242 - accuracy: 0.9171 - val_loss: 0.4528 - val_accuracy: 0.8600\n",
            "Epoch 234/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.2241 - accuracy: 0.9179 - val_loss: 0.7426 - val_accuracy: 0.7900\n",
            "Epoch 235/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.2313 - accuracy: 0.9111 - val_loss: 1.0895 - val_accuracy: 0.7383\n",
            "Epoch 236/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.2180 - accuracy: 0.9168 - val_loss: 0.6360 - val_accuracy: 0.7969\n",
            "Epoch 237/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.2193 - accuracy: 0.9191 - val_loss: 0.3936 - val_accuracy: 0.8759\n",
            "Epoch 238/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2144 - accuracy: 0.9205 - val_loss: 0.5112 - val_accuracy: 0.8491\n",
            "Epoch 239/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2219 - accuracy: 0.9191 - val_loss: 0.4131 - val_accuracy: 0.8719\n",
            "Epoch 240/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.2306 - accuracy: 0.9152 - val_loss: 0.5860 - val_accuracy: 0.8277\n",
            "Epoch 241/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.2126 - accuracy: 0.9229 - val_loss: 0.4090 - val_accuracy: 0.8689\n",
            "Epoch 242/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.2223 - accuracy: 0.9182 - val_loss: 0.8281 - val_accuracy: 0.7627\n",
            "Epoch 243/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2099 - accuracy: 0.9247 - val_loss: 0.5704 - val_accuracy: 0.8138\n",
            "Epoch 244/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2249 - accuracy: 0.9175 - val_loss: 0.5368 - val_accuracy: 0.8416\n",
            "Epoch 245/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2138 - accuracy: 0.9207 - val_loss: 0.4114 - val_accuracy: 0.8644\n",
            "Epoch 246/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2082 - accuracy: 0.9250 - val_loss: 0.4448 - val_accuracy: 0.8565\n",
            "Epoch 247/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2172 - accuracy: 0.9208 - val_loss: 0.9858 - val_accuracy: 0.7800\n",
            "Epoch 248/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2066 - accuracy: 0.9248 - val_loss: 0.7943 - val_accuracy: 0.7870\n",
            "Epoch 249/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2101 - accuracy: 0.9231 - val_loss: 0.4871 - val_accuracy: 0.8391\n",
            "Epoch 250/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2010 - accuracy: 0.9286 - val_loss: 2.0078 - val_accuracy: 0.6172\n",
            "Epoch 251/500\n",
            "567/567 [==============================] - 118s 209ms/step - loss: 0.2150 - accuracy: 0.9225 - val_loss: 0.4449 - val_accuracy: 0.8654\n",
            "Epoch 252/500\n",
            "567/567 [==============================] - 118s 209ms/step - loss: 0.1994 - accuracy: 0.9274 - val_loss: 0.5626 - val_accuracy: 0.8317\n",
            "Epoch 253/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.2043 - accuracy: 0.9235 - val_loss: 1.3868 - val_accuracy: 0.6971\n",
            "Epoch 254/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2021 - accuracy: 0.9276 - val_loss: 0.4746 - val_accuracy: 0.8550\n",
            "Epoch 255/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2110 - accuracy: 0.9216 - val_loss: 0.4896 - val_accuracy: 0.8610\n",
            "Epoch 256/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.1921 - accuracy: 0.9282 - val_loss: 0.4853 - val_accuracy: 0.8610\n",
            "Epoch 257/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.2030 - accuracy: 0.9258 - val_loss: 0.4320 - val_accuracy: 0.8605\n",
            "Epoch 258/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2051 - accuracy: 0.9234 - val_loss: 0.9411 - val_accuracy: 0.7552\n",
            "Epoch 259/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.2018 - accuracy: 0.9252 - val_loss: 1.2580 - val_accuracy: 0.7115\n",
            "Epoch 260/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.1948 - accuracy: 0.9292 - val_loss: 0.4235 - val_accuracy: 0.8620\n",
            "Epoch 261/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.2054 - accuracy: 0.9226 - val_loss: 1.8525 - val_accuracy: 0.6261\n",
            "Epoch 262/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1918 - accuracy: 0.9298 - val_loss: 0.4860 - val_accuracy: 0.8550\n",
            "Epoch 263/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2003 - accuracy: 0.9263 - val_loss: 0.3431 - val_accuracy: 0.8928\n",
            "Epoch 264/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.2069 - accuracy: 0.9248 - val_loss: 0.8017 - val_accuracy: 0.7994\n",
            "Epoch 265/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.1799 - accuracy: 0.9342 - val_loss: 0.5407 - val_accuracy: 0.8441\n",
            "Epoch 266/500\n",
            "567/567 [==============================] - 115s 202ms/step - loss: 0.2060 - accuracy: 0.9258 - val_loss: 0.6028 - val_accuracy: 0.8193\n",
            "Epoch 267/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.1894 - accuracy: 0.9322 - val_loss: 0.4641 - val_accuracy: 0.8630\n",
            "Epoch 268/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.1901 - accuracy: 0.9313 - val_loss: 0.5250 - val_accuracy: 0.8386\n",
            "Epoch 269/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.1905 - accuracy: 0.9297 - val_loss: 2.1756 - val_accuracy: 0.6236\n",
            "Epoch 270/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1823 - accuracy: 0.9319 - val_loss: 0.5644 - val_accuracy: 0.8386\n",
            "Epoch 271/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.1960 - accuracy: 0.9270 - val_loss: 0.8265 - val_accuracy: 0.7716\n",
            "Epoch 272/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1953 - accuracy: 0.9266 - val_loss: 0.5697 - val_accuracy: 0.8352\n",
            "Epoch 273/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1828 - accuracy: 0.9348 - val_loss: 2.1574 - val_accuracy: 0.6643\n",
            "Epoch 274/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1879 - accuracy: 0.9286 - val_loss: 1.1016 - val_accuracy: 0.7701\n",
            "Epoch 275/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1930 - accuracy: 0.9289 - val_loss: 0.4435 - val_accuracy: 0.8669\n",
            "Epoch 276/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1842 - accuracy: 0.9356 - val_loss: 0.3892 - val_accuracy: 0.8754\n",
            "Epoch 277/500\n",
            "567/567 [==============================] - 118s 209ms/step - loss: 0.1766 - accuracy: 0.9352 - val_loss: 0.3659 - val_accuracy: 0.8868\n",
            "Epoch 278/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1882 - accuracy: 0.9316 - val_loss: 0.4100 - val_accuracy: 0.8684\n",
            "Epoch 279/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1927 - accuracy: 0.9309 - val_loss: 0.8599 - val_accuracy: 0.7825\n",
            "Epoch 280/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1665 - accuracy: 0.9389 - val_loss: 0.3442 - val_accuracy: 0.8967\n",
            "Epoch 281/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1900 - accuracy: 0.9327 - val_loss: 0.8792 - val_accuracy: 0.7761\n",
            "Epoch 282/500\n",
            "567/567 [==============================] - 118s 207ms/step - loss: 0.1799 - accuracy: 0.9340 - val_loss: 0.4377 - val_accuracy: 0.8674\n",
            "Epoch 283/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1820 - accuracy: 0.9337 - val_loss: 0.4850 - val_accuracy: 0.8550\n",
            "Epoch 284/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1748 - accuracy: 0.9354 - val_loss: 0.8434 - val_accuracy: 0.7959\n",
            "Epoch 285/500\n",
            "567/567 [==============================] - 119s 209ms/step - loss: 0.1718 - accuracy: 0.9376 - val_loss: 0.4207 - val_accuracy: 0.8833\n",
            "Epoch 286/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1755 - accuracy: 0.9360 - val_loss: 0.5004 - val_accuracy: 0.8510\n",
            "Epoch 287/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.1921 - accuracy: 0.9312 - val_loss: 1.0826 - val_accuracy: 0.7214\n",
            "Epoch 288/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1660 - accuracy: 0.9392 - val_loss: 0.4140 - val_accuracy: 0.8649\n",
            "Epoch 289/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1811 - accuracy: 0.9330 - val_loss: 0.6528 - val_accuracy: 0.8312\n",
            "Epoch 290/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.1783 - accuracy: 0.9354 - val_loss: 0.3341 - val_accuracy: 0.8952\n",
            "Epoch 291/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.1665 - accuracy: 0.9375 - val_loss: 0.4123 - val_accuracy: 0.8774\n",
            "Epoch 292/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.1896 - accuracy: 0.9330 - val_loss: 0.3656 - val_accuracy: 0.8823\n",
            "Epoch 293/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1606 - accuracy: 0.9416 - val_loss: 0.4755 - val_accuracy: 0.8600\n",
            "Epoch 294/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.1637 - accuracy: 0.9391 - val_loss: 8.1081 - val_accuracy: 0.4245\n",
            "Epoch 295/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.1724 - accuracy: 0.9372 - val_loss: 0.3900 - val_accuracy: 0.8823\n",
            "Epoch 296/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.1809 - accuracy: 0.9331 - val_loss: 0.9546 - val_accuracy: 0.7542\n",
            "Epoch 297/500\n",
            "567/567 [==============================] - 111s 196ms/step - loss: 0.1629 - accuracy: 0.9407 - val_loss: 0.6256 - val_accuracy: 0.8247\n",
            "Epoch 298/500\n",
            "567/567 [==============================] - 110s 194ms/step - loss: 0.1677 - accuracy: 0.9377 - val_loss: 0.4379 - val_accuracy: 0.8620\n",
            "Epoch 299/500\n",
            "567/567 [==============================] - 110s 195ms/step - loss: 0.1693 - accuracy: 0.9378 - val_loss: 0.4337 - val_accuracy: 0.8644 0.17\n",
            "Epoch 300/500\n",
            "567/567 [==============================] - 110s 195ms/step - loss: 0.1642 - accuracy: 0.9404 - val_loss: 0.6608 - val_accuracy: 0.8183\n",
            "Epoch 301/500\n",
            "567/567 [==============================] - 111s 196ms/step - loss: 0.1787 - accuracy: 0.9363 - val_loss: 0.4353 - val_accuracy: 0.8719\n",
            "Epoch 302/500\n",
            "567/567 [==============================] - 110s 195ms/step - loss: 0.1710 - accuracy: 0.9375 - val_loss: 0.4313 - val_accuracy: 0.8620\n",
            "Epoch 303/500\n",
            "567/567 [==============================] - 111s 196ms/step - loss: 0.1652 - accuracy: 0.9395 - val_loss: 1.1617 - val_accuracy: 0.7289\n",
            "Epoch 304/500\n",
            "567/567 [==============================] - 111s 195ms/step - loss: 0.1698 - accuracy: 0.9385 - val_loss: 0.6094 - val_accuracy: 0.8357\n",
            "Epoch 305/500\n",
            "567/567 [==============================] - 110s 194ms/step - loss: 0.1676 - accuracy: 0.9385 - val_loss: 1.1891 - val_accuracy: 0.7483\n",
            "Epoch 306/500\n",
            "567/567 [==============================] - 110s 195ms/step - loss: 0.1756 - accuracy: 0.9367 - val_loss: 0.3673 - val_accuracy: 0.8823\n",
            "Epoch 307/500\n",
            "567/567 [==============================] - 110s 194ms/step - loss: 0.1629 - accuracy: 0.9412 - val_loss: 1.5568 - val_accuracy: 0.6688\n",
            "Epoch 308/500\n",
            "567/567 [==============================] - 110s 195ms/step - loss: 0.1575 - accuracy: 0.9424 - val_loss: 0.5406 - val_accuracy: 0.8530\n",
            "Epoch 309/500\n",
            "567/567 [==============================] - 110s 195ms/step - loss: 0.1565 - accuracy: 0.9432 - val_loss: 0.3503 - val_accuracy: 0.8997\n",
            "Epoch 310/500\n",
            "567/567 [==============================] - 110s 194ms/step - loss: 0.1717 - accuracy: 0.9374 - val_loss: 1.0489 - val_accuracy: 0.7602\n",
            "Epoch 311/500\n",
            "567/567 [==============================] - 110s 193ms/step - loss: 0.1682 - accuracy: 0.9386 - val_loss: 0.5815 - val_accuracy: 0.8491\n",
            "Epoch 312/500\n",
            "567/567 [==============================] - 110s 194ms/step - loss: 0.1572 - accuracy: 0.9429 - val_loss: 0.3219 - val_accuracy: 0.8932\n",
            "Epoch 313/500\n",
            "567/567 [==============================] - 110s 193ms/step - loss: 0.1698 - accuracy: 0.9392 - val_loss: 0.3486 - val_accuracy: 0.8908\n",
            "Epoch 314/500\n",
            "567/567 [==============================] - 110s 194ms/step - loss: 0.1552 - accuracy: 0.9448 - val_loss: 0.6191 - val_accuracy: 0.8406\n",
            "Epoch 315/500\n",
            "567/567 [==============================] - 111s 195ms/step - loss: 0.1642 - accuracy: 0.9401 - val_loss: 1.0354 - val_accuracy: 0.7418\n",
            "Epoch 316/500\n",
            "567/567 [==============================] - 110s 194ms/step - loss: 0.1518 - accuracy: 0.9440 - val_loss: 0.6611 - val_accuracy: 0.8168\n",
            "Epoch 317/500\n",
            "567/567 [==============================] - 110s 194ms/step - loss: 0.1571 - accuracy: 0.9426 - val_loss: 0.4365 - val_accuracy: 0.8803\n",
            "Epoch 318/500\n",
            "567/567 [==============================] - 111s 195ms/step - loss: 0.1562 - accuracy: 0.9434 - val_loss: 0.7653 - val_accuracy: 0.8198\n",
            "Epoch 319/500\n",
            "567/567 [==============================] - 110s 194ms/step - loss: 0.1505 - accuracy: 0.9455 - val_loss: 0.5556 - val_accuracy: 0.8535\n",
            "Epoch 320/500\n",
            "567/567 [==============================] - 110s 195ms/step - loss: 0.1706 - accuracy: 0.9399 - val_loss: 1.3999 - val_accuracy: 0.7105\n",
            "Epoch 321/500\n",
            "567/567 [==============================] - 110s 194ms/step - loss: 0.1520 - accuracy: 0.9453 - val_loss: 0.9268 - val_accuracy: 0.7989\n",
            "Epoch 322/500\n",
            "567/567 [==============================] - 109s 193ms/step - loss: 0.1520 - accuracy: 0.9444 - val_loss: 1.5754 - val_accuracy: 0.7071\n",
            "Epoch 323/500\n",
            "567/567 [==============================] - 110s 193ms/step - loss: 0.1543 - accuracy: 0.9415 - val_loss: 1.7838 - val_accuracy: 0.6887\n",
            "Epoch 324/500\n",
            "567/567 [==============================] - 109s 193ms/step - loss: 0.1558 - accuracy: 0.9433 - val_loss: 1.1622 - val_accuracy: 0.7602\n",
            "Epoch 325/500\n",
            "567/567 [==============================] - 110s 193ms/step - loss: 0.1499 - accuracy: 0.9445 - val_loss: 0.7393 - val_accuracy: 0.8128\n",
            "Epoch 326/500\n",
            "567/567 [==============================] - 114s 201ms/step - loss: 0.1558 - accuracy: 0.9422 - val_loss: 0.3575 - val_accuracy: 0.8893\n",
            "Epoch 327/500\n",
            "567/567 [==============================] - 112s 197ms/step - loss: 0.1605 - accuracy: 0.9391 - val_loss: 0.3996 - val_accuracy: 0.8759\n",
            "Epoch 328/500\n",
            "567/567 [==============================] - 108s 191ms/step - loss: 0.1420 - accuracy: 0.9496 - val_loss: 0.4681 - val_accuracy: 0.8654\n",
            "Epoch 329/500\n",
            "567/567 [==============================] - 108s 191ms/step - loss: 0.1437 - accuracy: 0.9479 - val_loss: 0.6653 - val_accuracy: 0.8237\n",
            "Epoch 330/500\n",
            "567/567 [==============================] - 110s 193ms/step - loss: 0.1441 - accuracy: 0.9496 - val_loss: 0.4606 - val_accuracy: 0.8734\n",
            "Epoch 331/500\n",
            "567/567 [==============================] - 106s 188ms/step - loss: 0.1491 - accuracy: 0.9459 - val_loss: 0.6320 - val_accuracy: 0.8461\n",
            "Epoch 332/500\n",
            "567/567 [==============================] - 107s 189ms/step - loss: 0.1522 - accuracy: 0.9440 - val_loss: 0.5027 - val_accuracy: 0.8570\n",
            "Epoch 333/500\n",
            "567/567 [==============================] - 108s 190ms/step - loss: 0.1579 - accuracy: 0.9443 - val_loss: 1.5945 - val_accuracy: 0.6912\n",
            "Epoch 334/500\n",
            "567/567 [==============================] - 109s 192ms/step - loss: 0.1498 - accuracy: 0.9458 - val_loss: 1.2893 - val_accuracy: 0.7547\n",
            "Epoch 335/500\n",
            "567/567 [==============================] - 106s 188ms/step - loss: 0.1389 - accuracy: 0.9522 - val_loss: 0.6843 - val_accuracy: 0.8039\n",
            "Epoch 336/500\n",
            "567/567 [==============================] - 111s 195ms/step - loss: 0.1582 - accuracy: 0.9444 - val_loss: 0.4355 - val_accuracy: 0.8684\n",
            "Epoch 337/500\n",
            "567/567 [==============================] - 114s 201ms/step - loss: 0.1510 - accuracy: 0.9447 - val_loss: 2.6405 - val_accuracy: 0.6509\n",
            "Epoch 338/500\n",
            "567/567 [==============================] - 110s 195ms/step - loss: 0.1461 - accuracy: 0.9461 - val_loss: 0.3432 - val_accuracy: 0.8928\n",
            "Epoch 339/500\n",
            "567/567 [==============================] - 109s 192ms/step - loss: 0.1401 - accuracy: 0.9500 - val_loss: 1.2620 - val_accuracy: 0.7205\n",
            "Epoch 340/500\n",
            "567/567 [==============================] - 109s 192ms/step - loss: 0.1518 - accuracy: 0.9443 - val_loss: 1.1314 - val_accuracy: 0.7577\n",
            "Epoch 341/500\n",
            "567/567 [==============================] - 110s 194ms/step - loss: 0.1392 - accuracy: 0.9503 - val_loss: 2.4481 - val_accuracy: 0.5943\n",
            "Epoch 342/500\n",
            "567/567 [==============================] - 111s 195ms/step - loss: 0.1421 - accuracy: 0.9486 - val_loss: 0.4063 - val_accuracy: 0.8838\n",
            "Epoch 343/500\n",
            "567/567 [==============================] - 109s 193ms/step - loss: 0.1512 - accuracy: 0.9458 - val_loss: 0.5810 - val_accuracy: 0.8466\n",
            "Epoch 344/500\n",
            "567/567 [==============================] - 904s 2s/step - loss: 0.1370 - accuracy: 0.9496 - val_loss: 0.6499 - val_accuracy: 0.8163\n",
            "Epoch 345/500\n",
            "567/567 [==============================] - 124s 219ms/step - loss: 0.1484 - accuracy: 0.9466 - val_loss: 0.4096 - val_accuracy: 0.8828\n",
            "Epoch 346/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.1316 - accuracy: 0.9539 - val_loss: 1.1528 - val_accuracy: 0.7498ss: 0.1370 - - ETA: 36s - loss: 0.1366 - accuracy: 0.95 - ETA: 26s - loss: 0.1350 - accura - ETA: 24s - loss: 0.1340 - accuracy: 0.95 - ETA: 24s - lo - ETA: 9s - loss: 0.1324 - accuracy - ETA: 8s - loss: 0.1321 - ac - ETA: 7s -\n",
            "Epoch 347/500\n",
            "567/567 [==============================] - 111s 196ms/step - loss: 0.1510 - accuracy: 0.9463 - val_loss: 2.3382 - val_accuracy: 0.6430\n",
            "Epoch 348/500\n",
            "567/567 [==============================] - 114s 200ms/step - loss: 0.1382 - accuracy: 0.9482 - val_loss: 0.3429 - val_accuracy: 0.9042\n",
            "Epoch 349/500\n",
            "567/567 [==============================] - 114s 200ms/step - loss: 0.1413 - accuracy: 0.9484 - val_loss: 0.4394 - val_accuracy: 0.8878\n",
            "Epoch 350/500\n",
            "567/567 [==============================] - 124s 219ms/step - loss: 0.1398 - accuracy: 0.9481 - val_loss: 0.7557 - val_accuracy: 0.8143\n",
            "Epoch 351/500\n",
            "567/567 [==============================] - 124s 219ms/step - loss: 0.1298 - accuracy: 0.9538 - val_loss: 0.3346 - val_accuracy: 0.9057\n",
            "Epoch 352/500\n",
            "567/567 [==============================] - 124s 219ms/step - loss: 0.1473 - accuracy: 0.9470 - val_loss: 0.7275 - val_accuracy: 0.8143\n",
            "Epoch 353/500\n",
            "567/567 [==============================] - 124s 219ms/step - loss: 0.1402 - accuracy: 0.9496 - val_loss: 1.1221 - val_accuracy: 0.7622\n",
            "Epoch 354/500\n",
            "567/567 [==============================] - 125s 220ms/step - loss: 0.1383 - accuracy: 0.9501 - val_loss: 1.1416 - val_accuracy: 0.7646\n",
            "Epoch 355/500\n",
            "567/567 [==============================] - 124s 218ms/step - loss: 0.1347 - accuracy: 0.9525 - val_loss: 0.5859 - val_accuracy: 0.8580\n",
            "Epoch 356/500\n",
            "567/567 [==============================] - 119s 210ms/step - loss: 0.1335 - accuracy: 0.9503 - val_loss: 0.4201 - val_accuracy: 0.8868\n",
            "Epoch 357/500\n",
            "567/567 [==============================] - 111s 196ms/step - loss: 0.1423 - accuracy: 0.9478 - val_loss: 0.2975 - val_accuracy: 0.9116: \n",
            "Epoch 358/500\n",
            "567/567 [==============================] - 111s 196ms/step - loss: 0.1379 - accuracy: 0.9495 - val_loss: 0.3866 - val_accuracy: 0.8903\n",
            "Epoch 359/500\n",
            "567/567 [==============================] - 112s 197ms/step - loss: 0.1532 - accuracy: 0.9456 - val_loss: 0.5473 - val_accuracy: 0.8694- loss: 0.1501 - accuracy: 0.946 - ETA: 16s - loss: 0.1506 - accuracy:  - ETA: 15s - loss: 0.1515 - ac - ETA: 12s - loss: 0.1530 - a - ETA: 9s - loss: 0.1537 - accuracy: 0.9 - ETA: 9s - loss: 0.1539 - accuracy: 0.94 - ETA: 9s - loss: 0.1542 -  - ETA: 7s - loss: 0.1546  - ETA: 1s - loss: 0.1\n",
            "Epoch 360/500\n",
            "567/567 [==============================] - 112s 197ms/step - loss: 0.1271 - accuracy: 0.9569 - val_loss: 0.4331 - val_accuracy: 0.8749\n",
            "Epoch 361/500\n",
            "567/567 [==============================] - 110s 194ms/step - loss: 0.1273 - accuracy: 0.9562 - val_loss: 0.4042 - val_accuracy: 0.8774\n",
            "Epoch 362/500\n",
            "567/567 [==============================] - 109s 193ms/step - loss: 0.1304 - accuracy: 0.9530 - val_loss: 0.5043 - val_accuracy: 0.8635\n",
            "Epoch 363/500\n",
            "567/567 [==============================] - 112s 197ms/step - loss: 0.1455 - accuracy: 0.9489 - val_loss: 0.3056 - val_accuracy: 0.9096\n",
            "Epoch 364/500\n",
            "567/567 [==============================] - 110s 194ms/step - loss: 0.1314 - accuracy: 0.9516 - val_loss: 0.5696 - val_accuracy: 0.8520\n",
            "Epoch 365/500\n",
            "567/567 [==============================] - 110s 194ms/step - loss: 0.1248 - accuracy: 0.9560 - val_loss: 0.4647 - val_accuracy: 0.8759ETA: 51s - loss: 0.1214 - accuracy: 0. - ETA: 50s - loss: 0. - ETA: 29s - loss: 0.1204 - accura - E - ETA: 19s - loss: 0.1232 - ETA - ETA: 0s - loss: 0.1250 - accuracy:  - ETA: 0s - loss: 0.1250 - accuracy: \n",
            "Epoch 366/500\n",
            "567/567 [==============================] - 110s 195ms/step - loss: 0.1378 - accuracy: 0.9519 - val_loss: 1.4457 - val_accuracy: 0.6996\n",
            "Epoch 367/500\n",
            "567/567 [==============================] - 110s 195ms/step - loss: 0.1373 - accuracy: 0.9512 - val_loss: 0.8435 - val_accuracy: 0.8024\n",
            "Epoch 368/500\n",
            "567/567 [==============================] - 109s 193ms/step - loss: 0.1292 - accuracy: 0.9545 - val_loss: 0.5226 - val_accuracy: 0.8496\n",
            "Epoch 369/500\n",
            "567/567 [==============================] - 113s 199ms/step - loss: 0.1301 - accuracy: 0.9525 - val_loss: 1.3329 - val_accuracy: 0.7234\n",
            "Epoch 370/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1350 - accuracy: 0.9516 - val_loss: 3.0886 - val_accuracy: 0.6137\n",
            "Epoch 371/500\n",
            "567/567 [==============================] - 118s 209ms/step - loss: 0.1318 - accuracy: 0.9529 - val_loss: 0.5696 - val_accuracy: 0.8654\n",
            "Epoch 372/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1264 - accuracy: 0.9533 - val_loss: 0.3704 - val_accuracy: 0.8913\n",
            "Epoch 373/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.1290 - accuracy: 0.9543 - val_loss: 0.3854 - val_accuracy: 0.8893\n",
            "Epoch 374/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.1283 - accuracy: 0.9527 - val_loss: 0.2925 - val_accuracy: 0.9186\n",
            "Epoch 375/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1329 - accuracy: 0.9533 - val_loss: 1.9216 - val_accuracy: 0.6480\n",
            "Epoch 376/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1311 - accuracy: 0.9522 - val_loss: 1.0771 - val_accuracy: 0.7681\n",
            "Epoch 377/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1398 - accuracy: 0.9493 - val_loss: 1.7398 - val_accuracy: 0.6966\n",
            "Epoch 378/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1310 - accuracy: 0.9535 - val_loss: 0.5464 - val_accuracy: 0.8510\n",
            "Epoch 379/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.1356 - accuracy: 0.9510 - val_loss: 0.3981 - val_accuracy: 0.8997\n",
            "Epoch 380/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1224 - accuracy: 0.9566 - val_loss: 0.3416 - val_accuracy: 0.9002\n",
            "Epoch 381/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1363 - accuracy: 0.9508 - val_loss: 0.4911 - val_accuracy: 0.8550\n",
            "Epoch 382/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1254 - accuracy: 0.9545 - val_loss: 0.4835 - val_accuracy: 0.8610\n",
            "Epoch 383/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.1158 - accuracy: 0.9577 - val_loss: 0.3745 - val_accuracy: 0.8918\n",
            "Epoch 384/500\n",
            "567/567 [==============================] - 118s 209ms/step - loss: 0.1411 - accuracy: 0.9496 - val_loss: 0.4823 - val_accuracy: 0.8679\n",
            "Epoch 385/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1255 - accuracy: 0.9535 - val_loss: 3.3909 - val_accuracy: 0.5750\n",
            "Epoch 386/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.1129 - accuracy: 0.9575 - val_loss: 0.3526 - val_accuracy: 0.8987\n",
            "Epoch 387/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.1320 - accuracy: 0.9535 - val_loss: 0.5558 - val_accuracy: 0.8456\n",
            "Epoch 388/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.1260 - accuracy: 0.9516 - val_loss: 0.6668 - val_accuracy: 0.8272\n",
            "Epoch 389/500\n",
            "567/567 [==============================] - 116s 204ms/step - loss: 0.1065 - accuracy: 0.9603 - val_loss: 0.3346 - val_accuracy: 0.8997\n",
            "Epoch 390/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.1207 - accuracy: 0.9571 - val_loss: 0.8085 - val_accuracy: 0.8158\n",
            "Epoch 391/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.1220 - accuracy: 0.9565 - val_loss: 1.2693 - val_accuracy: 0.7800\n",
            "Epoch 392/500\n",
            "567/567 [==============================] - 115s 203ms/step - loss: 0.1270 - accuracy: 0.9528 - val_loss: 0.8061 - val_accuracy: 0.8237\n",
            "Epoch 393/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1232 - accuracy: 0.9570 - val_loss: 0.2942 - val_accuracy: 0.9081\n",
            "Epoch 394/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1246 - accuracy: 0.9562 - val_loss: 0.5536 - val_accuracy: 0.8709\n",
            "Epoch 395/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1174 - accuracy: 0.9582 - val_loss: 2.2456 - val_accuracy: 0.6678\n",
            "Epoch 396/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1200 - accuracy: 0.9569 - val_loss: 0.7639 - val_accuracy: 0.8237\n",
            "Epoch 397/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1222 - accuracy: 0.9566 - val_loss: 0.4098 - val_accuracy: 0.8823\n",
            "Epoch 398/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1295 - accuracy: 0.9554 - val_loss: 0.7956 - val_accuracy: 0.8163\n",
            "Epoch 399/500\n",
            "567/567 [==============================] - 118s 207ms/step - loss: 0.1066 - accuracy: 0.9608 - val_loss: 0.3868 - val_accuracy: 0.8992\n",
            "Epoch 400/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1286 - accuracy: 0.9527 - val_loss: 0.4760 - val_accuracy: 0.8759\n",
            "Epoch 401/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1152 - accuracy: 0.9585 - val_loss: 2.1480 - val_accuracy: 0.6971\n",
            "Epoch 402/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1160 - accuracy: 0.9573 - val_loss: 1.5809 - val_accuracy: 0.7463\n",
            "Epoch 403/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1334 - accuracy: 0.9528 - val_loss: 0.3685 - val_accuracy: 0.9067\n",
            "Epoch 404/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1149 - accuracy: 0.9583 - val_loss: 0.4558 - val_accuracy: 0.8704\n",
            "Epoch 405/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1271 - accuracy: 0.9551 - val_loss: 0.3235 - val_accuracy: 0.9086\n",
            "Epoch 406/500\n",
            "567/567 [==============================] - 118s 209ms/step - loss: 0.1027 - accuracy: 0.9627 - val_loss: 0.6229 - val_accuracy: 0.8272\n",
            "Epoch 407/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.1110 - accuracy: 0.9603 - val_loss: 0.4570 - val_accuracy: 0.8848\n",
            "Epoch 408/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1272 - accuracy: 0.9543 - val_loss: 0.4827 - val_accuracy: 0.8729\n",
            "Epoch 409/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1148 - accuracy: 0.9587 - val_loss: 0.7387 - val_accuracy: 0.8088\n",
            "Epoch 410/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1126 - accuracy: 0.9598 - val_loss: 0.7596 - val_accuracy: 0.8352\n",
            "Epoch 411/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.1167 - accuracy: 0.9576 - val_loss: 0.3534 - val_accuracy: 0.8952\n",
            "Epoch 412/500\n",
            "567/567 [==============================] - 116s 205ms/step - loss: 0.1128 - accuracy: 0.9602 - val_loss: 0.3238 - val_accuracy: 0.9151\n",
            "Epoch 413/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1134 - accuracy: 0.9596 - val_loss: 0.5460 - val_accuracy: 0.8535\n",
            "Epoch 414/500\n",
            "567/567 [==============================] - 118s 207ms/step - loss: 0.1077 - accuracy: 0.9620 - val_loss: 0.8171 - val_accuracy: 0.8049\n",
            "Epoch 415/500\n",
            "567/567 [==============================] - 119s 211ms/step - loss: 0.1160 - accuracy: 0.9595 - val_loss: 0.3642 - val_accuracy: 0.8972\n",
            "Epoch 416/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1103 - accuracy: 0.9591 - val_loss: 0.3180 - val_accuracy: 0.9101\n",
            "Epoch 417/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1144 - accuracy: 0.9596 - val_loss: 0.3679 - val_accuracy: 0.8992\n",
            "Epoch 418/500\n",
            "567/567 [==============================] - 118s 209ms/step - loss: 0.1109 - accuracy: 0.9614 - val_loss: 0.3848 - val_accuracy: 0.8913\n",
            "Epoch 419/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1216 - accuracy: 0.9582 - val_loss: 0.5694 - val_accuracy: 0.8630\n",
            "Epoch 420/500\n",
            "567/567 [==============================] - 118s 207ms/step - loss: 0.1111 - accuracy: 0.9609 - val_loss: 0.6582 - val_accuracy: 0.8352\n",
            "Epoch 421/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1239 - accuracy: 0.9569 - val_loss: 0.4995 - val_accuracy: 0.8625\n",
            "Epoch 422/500\n",
            "567/567 [==============================] - 118s 209ms/step - loss: 0.1182 - accuracy: 0.9578 - val_loss: 0.4328 - val_accuracy: 0.8779\n",
            "Epoch 423/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1040 - accuracy: 0.9635 - val_loss: 0.3919 - val_accuracy: 0.8873\n",
            "Epoch 424/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1111 - accuracy: 0.9599 - val_loss: 1.5033 - val_accuracy: 0.7373\n",
            "Epoch 425/500\n",
            "567/567 [==============================] - 119s 209ms/step - loss: 0.1168 - accuracy: 0.9582 - val_loss: 4.6000 - val_accuracy: 0.5477\n",
            "Epoch 426/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1064 - accuracy: 0.9623 - val_loss: 0.8537 - val_accuracy: 0.8103\n",
            "Epoch 427/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1184 - accuracy: 0.9575 - val_loss: 0.4465 - val_accuracy: 0.8784\n",
            "Epoch 428/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1114 - accuracy: 0.9596 - val_loss: 0.3545 - val_accuracy: 0.9076\n",
            "Epoch 429/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1033 - accuracy: 0.9631 - val_loss: 0.3794 - val_accuracy: 0.8932\n",
            "Epoch 430/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1042 - accuracy: 0.9610 - val_loss: 0.6474 - val_accuracy: 0.8545\n",
            "Epoch 431/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1101 - accuracy: 0.9604 - val_loss: 0.4295 - val_accuracy: 0.9032\n",
            "Epoch 432/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1039 - accuracy: 0.9615 - val_loss: 0.3774 - val_accuracy: 0.8987\n",
            "Epoch 433/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1154 - accuracy: 0.9598 - val_loss: 2.0571 - val_accuracy: 0.6887\n",
            "Epoch 434/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1138 - accuracy: 0.9597 - val_loss: 5.4458 - val_accuracy: 0.5214\n",
            "Epoch 435/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1229 - accuracy: 0.9559 - val_loss: 0.4062 - val_accuracy: 0.8913\n",
            "Epoch 436/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1037 - accuracy: 0.9629 - val_loss: 2.5097 - val_accuracy: 0.6653\n",
            "Epoch 437/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.0958 - accuracy: 0.9651 - val_loss: 0.3826 - val_accuracy: 0.8923\n",
            "Epoch 438/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1059 - accuracy: 0.9622 - val_loss: 5.5460 - val_accuracy: 0.5134\n",
            "Epoch 439/500\n",
            "567/567 [==============================] - 119s 210ms/step - loss: 0.0981 - accuracy: 0.9644 - val_loss: 0.4641 - val_accuracy: 0.8853\n",
            "Epoch 440/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1153 - accuracy: 0.9585 - val_loss: 1.7819 - val_accuracy: 0.7244\n",
            "Epoch 441/500\n",
            "567/567 [==============================] - 119s 210ms/step - loss: 0.1261 - accuracy: 0.9557 - val_loss: 0.7216 - val_accuracy: 0.8416\n",
            "Epoch 442/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1176 - accuracy: 0.9583 - val_loss: 0.7998 - val_accuracy: 0.8262\n",
            "Epoch 443/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1062 - accuracy: 0.9624 - val_loss: 1.4307 - val_accuracy: 0.7468\n",
            "Epoch 444/500\n",
            "567/567 [==============================] - 119s 209ms/step - loss: 0.0922 - accuracy: 0.9680 - val_loss: 0.3865 - val_accuracy: 0.8932\n",
            "Epoch 445/500\n",
            "567/567 [==============================] - 119s 209ms/step - loss: 0.1173 - accuracy: 0.9586 - val_loss: 1.7777 - val_accuracy: 0.7100\n",
            "Epoch 446/500\n",
            "567/567 [==============================] - 119s 209ms/step - loss: 0.0970 - accuracy: 0.9650 - val_loss: 1.6246 - val_accuracy: 0.7319\n",
            "Epoch 447/500\n",
            "567/567 [==============================] - 118s 209ms/step - loss: 0.1095 - accuracy: 0.9611 - val_loss: 0.3941 - val_accuracy: 0.8883\n",
            "Epoch 448/500\n",
            "567/567 [==============================] - 118s 207ms/step - loss: 0.1004 - accuracy: 0.9648 - val_loss: 0.4924 - val_accuracy: 0.8858\n",
            "Epoch 449/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1134 - accuracy: 0.9589 - val_loss: 0.5167 - val_accuracy: 0.8625\n",
            "Epoch 450/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.0999 - accuracy: 0.9628 - val_loss: 0.5744 - val_accuracy: 0.8620\n",
            "Epoch 451/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1095 - accuracy: 0.9614 - val_loss: 0.4437 - val_accuracy: 0.8848\n",
            "Epoch 452/500\n",
            "567/567 [==============================] - 118s 209ms/step - loss: 0.1019 - accuracy: 0.9647 - val_loss: 3.7590 - val_accuracy: 0.5621\n",
            "Epoch 453/500\n",
            "567/567 [==============================] - 120s 211ms/step - loss: 0.1008 - accuracy: 0.9645 - val_loss: 0.4915 - val_accuracy: 0.8679\n",
            "Epoch 454/500\n",
            "567/567 [==============================] - 121s 213ms/step - loss: 0.0991 - accuracy: 0.9650 - val_loss: 0.4729 - val_accuracy: 0.8843\n",
            "Epoch 455/500\n",
            "567/567 [==============================] - 118s 207ms/step - loss: 0.1055 - accuracy: 0.9618 - val_loss: 0.4981 - val_accuracy: 0.8818\n",
            "Epoch 456/500\n",
            "567/567 [==============================] - 119s 209ms/step - loss: 0.1005 - accuracy: 0.9658 - val_loss: 1.4732 - val_accuracy: 0.7200\n",
            "Epoch 457/500\n",
            "567/567 [==============================] - 118s 207ms/step - loss: 0.1086 - accuracy: 0.9605 - val_loss: 0.7317 - val_accuracy: 0.8282\n",
            "Epoch 458/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1019 - accuracy: 0.9646 - val_loss: 0.4447 - val_accuracy: 0.8779\n",
            "Epoch 459/500\n",
            "567/567 [==============================] - 117s 206ms/step - loss: 0.1094 - accuracy: 0.9608 - val_loss: 0.5721 - val_accuracy: 0.8500\n",
            "Epoch 460/500\n",
            "567/567 [==============================] - 117s 207ms/step - loss: 0.1011 - accuracy: 0.9641 - val_loss: 3.3087 - val_accuracy: 0.6068\n",
            "Epoch 461/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.0955 - accuracy: 0.9640 - val_loss: 5.0198 - val_accuracy: 0.5437\n",
            "Epoch 462/500\n",
            "567/567 [==============================] - 120s 212ms/step - loss: 0.0997 - accuracy: 0.9658 - val_loss: 0.4285 - val_accuracy: 0.8833\n",
            "Epoch 463/500\n",
            "567/567 [==============================] - 119s 210ms/step - loss: 0.1022 - accuracy: 0.9629 - val_loss: 0.9860 - val_accuracy: 0.8158\n",
            "Epoch 464/500\n",
            "567/567 [==============================] - 120s 211ms/step - loss: 0.1025 - accuracy: 0.9638 - val_loss: 0.4915 - val_accuracy: 0.8654\n",
            "Epoch 465/500\n",
            "567/567 [==============================] - 119s 211ms/step - loss: 0.0977 - accuracy: 0.9667 - val_loss: 1.6936 - val_accuracy: 0.7090\n",
            "Epoch 466/500\n",
            "567/567 [==============================] - 119s 210ms/step - loss: 0.1093 - accuracy: 0.9604 - val_loss: 0.8519 - val_accuracy: 0.8029\n",
            "Epoch 467/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.0978 - accuracy: 0.9652 - val_loss: 0.7886 - val_accuracy: 0.8173\n",
            "Epoch 468/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.1012 - accuracy: 0.9630 - val_loss: 0.6748 - val_accuracy: 0.8505\n",
            "Epoch 469/500\n",
            "567/567 [==============================] - 118s 208ms/step - loss: 0.0981 - accuracy: 0.9642 - val_loss: 1.0500 - val_accuracy: 0.7989\n",
            "Epoch 470/500\n",
            "567/567 [==============================] - 120s 212ms/step - loss: 0.1058 - accuracy: 0.9641 - val_loss: 0.4544 - val_accuracy: 0.8962\n",
            "Epoch 471/500\n",
            "567/567 [==============================] - 120s 211ms/step - loss: 0.0970 - accuracy: 0.9653 - val_loss: 3.6877 - val_accuracy: 0.6271\n",
            "Epoch 472/500\n",
            "567/567 [==============================] - 121s 213ms/step - loss: 0.0953 - accuracy: 0.9655 - val_loss: 4.0047 - val_accuracy: 0.5859\n",
            "Epoch 473/500\n",
            "567/567 [==============================] - 120s 212ms/step - loss: 0.1044 - accuracy: 0.9640 - val_loss: 1.3205 - val_accuracy: 0.7830\n",
            "Epoch 474/500\n",
            "567/567 [==============================] - 120s 212ms/step - loss: 0.0914 - accuracy: 0.9671 - val_loss: 0.5747 - val_accuracy: 0.8635\n",
            "Epoch 475/500\n",
            "567/567 [==============================] - 119s 210ms/step - loss: 0.1037 - accuracy: 0.9629 - val_loss: 1.7517 - val_accuracy: 0.7269\n",
            "Epoch 476/500\n",
            "567/567 [==============================] - 119s 211ms/step - loss: 0.0909 - accuracy: 0.9692 - val_loss: 0.8421 - val_accuracy: 0.8188\n",
            "Epoch 477/500\n",
            "567/567 [==============================] - 119s 211ms/step - loss: 0.0911 - accuracy: 0.9677 - val_loss: 3.8613 - val_accuracy: 0.5839\n",
            "Epoch 478/500\n",
            "567/567 [==============================] - 120s 212ms/step - loss: 0.1166 - accuracy: 0.9605 - val_loss: 0.3676 - val_accuracy: 0.9022\n",
            "Epoch 479/500\n",
            "567/567 [==============================] - 121s 213ms/step - loss: 0.0964 - accuracy: 0.9655 - val_loss: 0.4319 - val_accuracy: 0.8942\n",
            "Epoch 480/500\n",
            "567/567 [==============================] - 119s 211ms/step - loss: 0.0952 - accuracy: 0.9656 - val_loss: 0.5595 - val_accuracy: 0.8759\n",
            "Epoch 481/500\n",
            "567/567 [==============================] - 119s 211ms/step - loss: 0.0963 - accuracy: 0.9679 - val_loss: 0.3111 - val_accuracy: 0.9181\n",
            "Epoch 482/500\n",
            "567/567 [==============================] - 121s 213ms/step - loss: 0.0860 - accuracy: 0.9695 - val_loss: 1.5603 - val_accuracy: 0.7696\n",
            "Epoch 483/500\n",
            "567/567 [==============================] - 119s 210ms/step - loss: 0.1002 - accuracy: 0.9637 - val_loss: 0.3814 - val_accuracy: 0.9022\n",
            "Epoch 484/500\n",
            "567/567 [==============================] - 119s 209ms/step - loss: 0.0831 - accuracy: 0.9703 - val_loss: 2.6599 - val_accuracy: 0.6728\n",
            "Epoch 485/500\n",
            "567/567 [==============================] - 119s 210ms/step - loss: 0.1168 - accuracy: 0.9596 - val_loss: 0.2885 - val_accuracy: 0.9250\n",
            "Epoch 486/500\n",
            "567/567 [==============================] - 120s 211ms/step - loss: 0.0923 - accuracy: 0.9684 - val_loss: 0.4639 - val_accuracy: 0.8913\n",
            "Epoch 487/500\n",
            "567/567 [==============================] - 120s 212ms/step - loss: 0.0973 - accuracy: 0.9661 - val_loss: 0.3887 - val_accuracy: 0.9017\n",
            "Epoch 488/500\n",
            "567/567 [==============================] - 121s 214ms/step - loss: 0.1071 - accuracy: 0.9624 - val_loss: 1.1267 - val_accuracy: 0.7979\n",
            "Epoch 489/500\n",
            "567/567 [==============================] - 120s 212ms/step - loss: 0.0876 - accuracy: 0.9678 - val_loss: 0.3577 - val_accuracy: 0.9067\n",
            "Epoch 490/500\n",
            "567/567 [==============================] - 119s 209ms/step - loss: 0.0968 - accuracy: 0.9650 - val_loss: 1.4158 - val_accuracy: 0.7517\n",
            "Epoch 491/500\n",
            "567/567 [==============================] - 120s 212ms/step - loss: 0.0919 - accuracy: 0.9656 - val_loss: 0.7113 - val_accuracy: 0.8282\n",
            "Epoch 492/500\n",
            "567/567 [==============================] - 119s 209ms/step - loss: 0.1004 - accuracy: 0.9652 - val_loss: 0.7877 - val_accuracy: 0.8352\n",
            "Epoch 493/500\n",
            "567/567 [==============================] - 121s 213ms/step - loss: 0.1052 - accuracy: 0.9630 - val_loss: 4.1743 - val_accuracy: 0.5650\n",
            "Epoch 494/500\n",
            "567/567 [==============================] - 121s 214ms/step - loss: 0.0859 - accuracy: 0.9709 - val_loss: 0.3912 - val_accuracy: 0.8997\n",
            "Epoch 495/500\n",
            "567/567 [==============================] - 121s 213ms/step - loss: 0.0843 - accuracy: 0.9706 - val_loss: 0.4930 - val_accuracy: 0.8704\n",
            "Epoch 496/500\n",
            "567/567 [==============================] - 121s 214ms/step - loss: 0.1110 - accuracy: 0.9612 - val_loss: 2.9018 - val_accuracy: 0.6425\n",
            "Epoch 497/500\n",
            "567/567 [==============================] - 122s 215ms/step - loss: 0.0857 - accuracy: 0.9689 - val_loss: 0.3070 - val_accuracy: 0.9206\n",
            "Epoch 498/500\n",
            "567/567 [==============================] - 123s 216ms/step - loss: 0.1049 - accuracy: 0.9640 - val_loss: 0.7498 - val_accuracy: 0.8213\n",
            "Epoch 499/500\n",
            "567/567 [==============================] - 122s 216ms/step - loss: 0.0843 - accuracy: 0.9703 - val_loss: 0.6000 - val_accuracy: 0.8515\n",
            "Epoch 500/500\n",
            "567/567 [==============================] - 121s 214ms/step - loss: 0.0999 - accuracy: 0.9636 - val_loss: 0.6475 - val_accuracy: 0.8451\n"
          ]
        }
      ],
      "source": [
        "model_train = model.fit(X_train, Y_train, batch_size=batch_size,epochs=epochs ,\n",
        "                        callbacks=[callbacks],\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_Val, Y_Val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DedgNPzIqh_"
      },
      "source": [
        "## Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOythe5lEH3E"
      },
      "outputs": [],
      "source": [
        "##load the saved model\n",
        "from tensorflow.keras import models\n",
        "saved_models=model.load_weights('D:/SaraYounas/SavedModels/AIR-CNN-model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SefQWJCwxtq"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        #layers.Resizing(image_size, image_size),\n",
        "        # layers.RandomFlip(\"horizontal\"),\n",
        "        # layers.RandomRotation(factor=0.02),\n",
        "        # layers.RandomZoom(\n",
        "        #     height_factor=0.2, width_factor=0.2\n",
        "\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS_iqGRZlCnX"
      },
      "source": [
        "## Evaluate Model On Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zAtFpeylCnq",
        "outputId": "0025707c-f291-49fe-c83c-5d0cd227d69b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.33908721804618835\n",
            "Test accuracy: 0.916385293006897\n"
          ]
        }
      ],
      "source": [
        "## test loss and accuracy\n",
        "test_eval = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test loss:', test_eval[0])\n",
        "print('Test accuracy:', test_eval[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx-UqVinlCnr",
        "outputId": "32e23902-c55f-4271-f543-2ddcd1e32bae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation on test data:  91.6385293006897\n"
          ]
        }
      ],
      "source": [
        "# test accuracy\n",
        "pred=model.predict(X_test)\n",
        "y_pred=np.argmax(pred,axis=1)\n",
        "\n",
        "loss, acc = model.evaluate(X_test, Y_test, verbose = 0)\n",
        "print(\"\\nEvaluation on test data: \", acc * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf9mZr1qlCns"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "    \"\"\"\n",
        "    given a sklearn confusion matrix (cm), make a nice plot\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
        "\n",
        "    target_names: given classification classes such as [0, 1, 2]\n",
        "                  the class names, for example: ['high', 'medium', 'low']\n",
        "\n",
        "    title:        the text to display at the top of the matrix\n",
        "\n",
        "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
        "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "                  plt.get_cmap('jet') or plt.cm.Blues\n",
        "\n",
        "    normalize:    If False, plot the raw numbers\n",
        "                  If True, plot the proportions\n",
        "\n",
        "    Usage\n",
        "    -----\n",
        "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
        "                                                              # sklearn.metrics.confusion_matrix\n",
        "                          normalize    = True,                # show proportions\n",
        "                          target_names = y_labels_vals,       # list of names of the classes\n",
        "                          title     A   = best_estimator_name) # title of graph\n",
        "\n",
        "    Citiation\n",
        "    ---------\n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "UMRcty5HlCnt",
        "outputId": "6430775c-d556-4c63-ecfa-094197e6ccc2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "confusion_mtx = confusion_matrix(Y_test, y_pred)\n",
        "cm_plot_labels = ['AK', 'BCC' ,'BKL', 'DF', 'SCC' ,'VASC' ,'MEL', 'NV']\n",
        "cm = plot_confusion_matrix(confusion_mtx, target_names = cm_plot_labels, normalize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0ZaKuhFgVNG",
        "outputId": "4aa0efac-dfdb-4dee-8162-0e881e93ce22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9163853028798411\n",
            "Confusion matrix: tf.Tensor(\n",
            "[[651  12   6   0   3   1   4   0]\n",
            " [ 30 610  12  21  13   6  24   1]\n",
            " [ 17  14 587   1  26  11   8   0]\n",
            " [  0   1   1 519   1   4   1   0]\n",
            " [  7   6  19   2 538  48   9   1]\n",
            " [  1   9   9   8  56 554   5   2]\n",
            " [  7   0   1   4   0   6 642   1]\n",
            " [  0   0   0   0   0   1   1 513]], shape=(8, 8), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.math import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#actual =  valid_data.labels\n",
        "preds = np.argmax(model.predict(X_test), axis=1)\n",
        "cfmx = confusion_matrix(Y_test, preds)\n",
        "acc = accuracy_score(Y_test,preds)\n",
        "print ('Test Accuracy:', acc )\n",
        "print('Confusion matrix:', cfmx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BZzyj-UgVNG",
        "outputId": "726ab17e-818f-43e7-8523-b78450b848bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.94       677\n",
            "           1       0.94      0.85      0.89       717\n",
            "           2       0.92      0.88      0.90       664\n",
            "           3       0.94      0.98      0.96       527\n",
            "           4       0.84      0.85      0.85       630\n",
            "           5       0.88      0.86      0.87       644\n",
            "           6       0.93      0.97      0.95       661\n",
            "           7       0.99      1.00      0.99       515\n",
            "\n",
            "    accuracy                           0.92      5035\n",
            "   macro avg       0.92      0.92      0.92      5035\n",
            "weighted avg       0.92      0.92      0.92      5035\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(Y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rlel5nqygVNH",
        "outputId": "7104b720-aee2-4d2e-ebf2-3d6dbd03e871"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.916\n",
            "Recall: 0.916\n",
            "F-Measure: 0.916\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "precision = precision_score(Y_test, y_pred, average='weighted')\n",
        "print('Precision: %.3f' % precision)\n",
        "recall = recall_score(Y_test, y_pred, average='weighted')\n",
        "print('Recall: %.3f' % recall)\n",
        "score = f1_score(Y_test, y_pred, average='weighted')\n",
        "print('F-Measure: %.3f' % score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "F1CtRpW3NCth",
        "4DedgNPzIqh_",
        "YS_iqGRZlCnX",
        "m3SRLpLj9bda",
        "0zmu05uDW-9H",
        "7CtOIwxPA7Wm",
        "pCkrc8nJB8ja",
        "RFF_p-axF0qH",
        "eUYzGMrmGdQ3",
        "qfKB69ASHjwk"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}